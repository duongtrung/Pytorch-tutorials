{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f54d53a0-db45-4476-96d4-0500ee4fae35",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/duongtrung/Pytorch-tutorials/blob/main/03_pytorch_computer_vision_up.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8961ee9-1eb8-4686-a18a-97c6f057eda3",
   "metadata": {},
   "source": [
    "# 03. PyTorch Computer Vision\n",
    "\n",
    "[Computer vision](https://en.wikipedia.org/wiki/Computer_vision) is the art of teaching a computer to see.\n",
    "\n",
    "For example, it could involve building a model to classify whether a photo is of a cat or a dog ([binary classification](https://developers.google.com/machine-learning/glossary#binary-classification)).\n",
    "\n",
    "Or whether a photo is of a cat, dog or chicken ([multi-class classification](https://developers.google.com/machine-learning/glossary#multi-class-classification)).\n",
    "\n",
    "Or identifying where a car appears in a video frame ([object detection](https://en.wikipedia.org/wiki/Object_detection)).\n",
    "\n",
    "Or figuring out where different objects in an image can be separated ([panoptic segmentation](https://arxiv.org/abs/1801.00868)).\n",
    "\n",
    "![example computer vision problems](resources/03-computer-vision-problems.png)\n",
    "*Example computer vision problems for binary classification, multiclass classification, object detection and segmentation.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6ffb8d-ddff-4ccc-b1bf-b89ba7d38e89",
   "metadata": {},
   "source": [
    "## What we're going to cover\n",
    "\n",
    "We're going to apply the PyTorch Workflow we've been learning in the past couple of sections to computer vision.\n",
    "\n",
    "![a PyTorch workflow with a computer vision focus](resources/03-pytorch-computer-vision-workflow.png)\n",
    "\n",
    "Specifically, we're going to cover:\n",
    "\n",
    "| **Topic** | **Contents** |\n",
    "| ----- | ----- |\n",
    "| **0. Computer vision libraries in PyTorch** | PyTorch has a bunch of built-in helpful computer vision libraries, let's check them out.  |\n",
    "| **1. Load data** | To practice computer vision, we'll start with some images of different pieces of clothing from [FashionMNIST](https://github.com/zalandoresearch/fashion-mnist). |\n",
    "| **2. Prepare data** | We've got some images, let's load them in with a [PyTorch `DataLoader`](https://pytorch.org/docs/stable/data.html) so we can use them with our training loop. |\n",
    "| **3. Model 0: Building a baseline model** | Here we'll create a multi-class classification model to learn patterns in the data, we'll also choose a **loss function**, **optimizer** and build a **training loop**. | \n",
    "| **4. Making predictions and evaluting model 0** | Let's make some predictions with our baseline model and evaluate them. |\n",
    "| **5. Setup device agnostic code for future models** | It's best practice to write device-agnostic code, so let's set it up. |\n",
    "| **6. Model 1: Adding non-linearity** | Experimenting is a large part of machine learning, let's try and improve upon our baseline model by adding non-linear layers. |\n",
    "| **7. Model 2: Convolutional Neural Network (CNN)** | Time to get computer vision specific and introduce the powerful convolutional neural network architecture. |\n",
    "| **8. Comparing our models** | We've built three different models, let's compare them. |\n",
    "| **9. Evaluating our best model** | Let's make some predictons on random images and evaluate our best model. |\n",
    "| **10. Making a confusion matrix** | A confusion matrix is a great way to evaluate a classification model, let's see how we can make one. |\n",
    "| **11. Saving and loading the best performing model** | Since we might want to use our model for later, let's save it and make sure it loads back in correctly. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad87baa-5c5d-4964-a635-a4d88e4371e3",
   "metadata": {},
   "source": [
    "## 0. Computer vision libraries in PyTorch\n",
    "\n",
    "Before we get started writing code, let's talk about some PyTorch computer vision libraries you should be aware of.\n",
    "\n",
    "| PyTorch module | What does it do? |\n",
    "| ----- | ----- |\n",
    "| [`torchvision`](https://pytorch.org/vision/stable/index.html) | Contains datasets, model architectures and image transformations often used for computer vision problems. |\n",
    "| [`torchvision.datasets`](https://pytorch.org/vision/stable/datasets.html) | Here you'll find many example computer vision datasets for a range of problems from image classification, object detection, image captioning, video classification and more. It also contains [a series of base classes for making custom datasets](https://pytorch.org/vision/stable/datasets.html#base-classes-for-custom-datasets). |\n",
    "| [`torchvision.models`](https://pytorch.org/vision/stable/models.html) | This module contains well-performing and commonly used computer vision model architectures implemented in PyTorch, you can use these with your own problems. | \n",
    "| [`torchvision.transforms`](https://pytorch.org/vision/stable/transforms.html) | Often images need to be transformed (turned into numbers/processed/augmented) before being used with a model, common image transformations are found here. | \n",
    "| [`torch.utils.data.Dataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) | Base dataset class for PyTorch.  | \n",
    "| [`torch.utils.data.DataLoader`](https://pytorch.org/docs/stable/data.html#module-torch.utils.data) | Creates a Python iteralbe over a dataset (created with `torch.utils.data.Dataset`). |\n",
    "\n",
    "> **Note:** The `torch.utils.data.Dataset` and `torch.utils.data.DataLoader` classes aren't only for computer vision in PyTorch, they are capable of dealing with many different types of data.\n",
    "\n",
    "Now we've covered some of the most important PyTorch computer vision libraries, let's import the relevant dependencies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "013b28f8-9172-444a-8536-98458c04a851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 1.12.1\n",
      "torchvision version: 0.13.1\n"
     ]
    }
   ],
   "source": [
    "# Import PyTorch\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# Import torchvision \n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "# Import matplotlib for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check versions\n",
    "# Note: your PyTorch version shouldn't be lower than 1.10.0 and torchvision version shouldn't be lower than 0.11\n",
    "print(f\"PyTorch version: {torch.__version__}\\ntorchvision version: {torchvision.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe61c37-6030-4174-a0bf-f240c70f94c7",
   "metadata": {},
   "source": [
    "## 1. Getting a dataset\n",
    "\n",
    "To begin working on a computer vision problem, let's get a computer vision dataset.\n",
    "\n",
    "We're going to start with FashionMNIST.\n",
    "\n",
    "MNIST stands for Modified National Institute of Standards and Technology.\n",
    "\n",
    "The [original MNIST dataset](https://en.wikipedia.org/wiki/MNIST_database) contains thousands of examples of handwritten digits (from 0 to 9) and was used to build computer vision models to identify numbers for postal services.\n",
    "\n",
    "[FashionMNIST](https://github.com/zalandoresearch/fashion-mnist), made by Zalando Research, is a similar setup. \n",
    "\n",
    "Except it contains grayscale images of 10 different kinds of clothing.\n",
    "\n",
    "![example image of FashionMNIST](resources/03-fashion-mnist-slide.png)\n",
    "*`torchvision.datasets` contains a lot of example datasets you can use to practice writing computer vision code on. FashionMNIST is one of those datasets. And since it has 10 different image classes (different types of clothing), it's a multi-class classification problem.*\n",
    "\n",
    "Later, we'll be building a computer vision neural network to identify the different styles of clothing in these images.\n",
    "\n",
    "PyTorch has a bunch of common computer vision datasets stored in `torchvision.datasets`.\n",
    "\n",
    "Including FashionMNIST in [`torchvision.datasets.FashionMNIST()`](https://pytorch.org/vision/main/generated/torchvision.datasets.FashionMNIST.html).\n",
    "\n",
    "To download it, we provide the following parameters:\n",
    "* `root: str` - which folder do you want to download the data to?\n",
    "* `train: Bool` - do you want the training or test split?\n",
    "* `download: Bool` - should the data be downloaded?\n",
    "* `transform: torchvision.transforms` - what transformations would you like to do on the data?\n",
    "* `target_transform` - you can transform the targets (labels) if you like too.\n",
    "\n",
    "Many other datasets in `torchvision` have these parameter options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0b01591-0f3a-48b0-a6dd-d1309b4afa78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup training data\n",
    "train_data = datasets.FashionMNIST(\n",
    "    root=\"D:\\gdrive\\DataCenter\\data\", # where to download data to? replace it with your data folder\n",
    "    train=True, # get training data\n",
    "    download=True, # download data if it doesn't exist on disk\n",
    "    transform=ToTensor(), # images come as PIL format, we want to turn into Torch tensors\n",
    "    target_transform=None # you can transform labels as well\n",
    ")\n",
    "\n",
    "# Setup testing data\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"D:\\gdrive\\DataCenter\\data\", # replace it with your data folder\n",
    "    train=False, # get test data\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1b3b6ae-c712-4ea6-a00b-e22a1ae30027",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000, 0.0510,\n",
       "           0.2863, 0.0000, 0.0000, 0.0039, 0.0157, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0039, 0.0039, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.0000, 0.1412, 0.5333,\n",
       "           0.4980, 0.2431, 0.2118, 0.0000, 0.0000, 0.0000, 0.0039, 0.0118,\n",
       "           0.0157, 0.0000, 0.0000, 0.0118],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0235, 0.0000, 0.4000, 0.8000,\n",
       "           0.6902, 0.5255, 0.5647, 0.4824, 0.0902, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0471, 0.0392, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6078, 0.9255,\n",
       "           0.8118, 0.6980, 0.4196, 0.6118, 0.6314, 0.4275, 0.2510, 0.0902,\n",
       "           0.3020, 0.5098, 0.2824, 0.0588],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.2706, 0.8118, 0.8745,\n",
       "           0.8549, 0.8471, 0.8471, 0.6392, 0.4980, 0.4745, 0.4784, 0.5725,\n",
       "           0.5529, 0.3451, 0.6745, 0.2588],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0039, 0.0039, 0.0039, 0.0000, 0.7843, 0.9098, 0.9098,\n",
       "           0.9137, 0.8980, 0.8745, 0.8745, 0.8431, 0.8353, 0.6431, 0.4980,\n",
       "           0.4824, 0.7686, 0.8980, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7176, 0.8824, 0.8471,\n",
       "           0.8745, 0.8941, 0.9216, 0.8902, 0.8784, 0.8706, 0.8784, 0.8667,\n",
       "           0.8745, 0.9608, 0.6784, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7569, 0.8941, 0.8549,\n",
       "           0.8353, 0.7765, 0.7059, 0.8314, 0.8235, 0.8275, 0.8353, 0.8745,\n",
       "           0.8627, 0.9529, 0.7922, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0039, 0.0118, 0.0000, 0.0471, 0.8588, 0.8627, 0.8314,\n",
       "           0.8549, 0.7529, 0.6627, 0.8902, 0.8157, 0.8549, 0.8784, 0.8314,\n",
       "           0.8863, 0.7725, 0.8196, 0.2039],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0235, 0.0000, 0.3882, 0.9569, 0.8706, 0.8627,\n",
       "           0.8549, 0.7961, 0.7765, 0.8667, 0.8431, 0.8353, 0.8706, 0.8627,\n",
       "           0.9608, 0.4667, 0.6549, 0.2196],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0157, 0.0000, 0.0000, 0.2157, 0.9255, 0.8941, 0.9020,\n",
       "           0.8941, 0.9412, 0.9098, 0.8353, 0.8549, 0.8745, 0.9176, 0.8510,\n",
       "           0.8510, 0.8196, 0.3608, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0039, 0.0157, 0.0235, 0.0275, 0.0078, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.9294, 0.8863, 0.8510, 0.8745,\n",
       "           0.8706, 0.8588, 0.8706, 0.8667, 0.8471, 0.8745, 0.8980, 0.8431,\n",
       "           0.8549, 1.0000, 0.3020, 0.0000],\n",
       "          [0.0000, 0.0118, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.2431, 0.5686, 0.8000, 0.8941, 0.8118, 0.8353, 0.8667,\n",
       "           0.8549, 0.8157, 0.8275, 0.8549, 0.8784, 0.8745, 0.8588, 0.8431,\n",
       "           0.8784, 0.9569, 0.6235, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.1725, 0.3216, 0.4196,\n",
       "           0.7412, 0.8941, 0.8627, 0.8706, 0.8510, 0.8863, 0.7843, 0.8039,\n",
       "           0.8275, 0.9020, 0.8784, 0.9176, 0.6902, 0.7373, 0.9804, 0.9725,\n",
       "           0.9137, 0.9333, 0.8431, 0.0000],\n",
       "          [0.0000, 0.2235, 0.7333, 0.8157, 0.8784, 0.8667, 0.8784, 0.8157,\n",
       "           0.8000, 0.8392, 0.8157, 0.8196, 0.7843, 0.6235, 0.9608, 0.7569,\n",
       "           0.8078, 0.8745, 1.0000, 1.0000, 0.8667, 0.9176, 0.8667, 0.8275,\n",
       "           0.8627, 0.9098, 0.9647, 0.0000],\n",
       "          [0.0118, 0.7922, 0.8941, 0.8784, 0.8667, 0.8275, 0.8275, 0.8392,\n",
       "           0.8039, 0.8039, 0.8039, 0.8627, 0.9412, 0.3137, 0.5882, 1.0000,\n",
       "           0.8980, 0.8667, 0.7373, 0.6039, 0.7490, 0.8235, 0.8000, 0.8196,\n",
       "           0.8706, 0.8941, 0.8824, 0.0000],\n",
       "          [0.3843, 0.9137, 0.7765, 0.8235, 0.8706, 0.8980, 0.8980, 0.9176,\n",
       "           0.9765, 0.8627, 0.7608, 0.8431, 0.8510, 0.9451, 0.2549, 0.2863,\n",
       "           0.4157, 0.4588, 0.6588, 0.8588, 0.8667, 0.8431, 0.8510, 0.8745,\n",
       "           0.8745, 0.8784, 0.8980, 0.1137],\n",
       "          [0.2941, 0.8000, 0.8314, 0.8000, 0.7569, 0.8039, 0.8275, 0.8824,\n",
       "           0.8471, 0.7255, 0.7725, 0.8078, 0.7765, 0.8353, 0.9412, 0.7647,\n",
       "           0.8902, 0.9608, 0.9373, 0.8745, 0.8549, 0.8314, 0.8196, 0.8706,\n",
       "           0.8627, 0.8667, 0.9020, 0.2627],\n",
       "          [0.1882, 0.7961, 0.7176, 0.7608, 0.8353, 0.7725, 0.7255, 0.7451,\n",
       "           0.7608, 0.7529, 0.7922, 0.8392, 0.8588, 0.8667, 0.8627, 0.9255,\n",
       "           0.8824, 0.8471, 0.7804, 0.8078, 0.7294, 0.7098, 0.6941, 0.6745,\n",
       "           0.7098, 0.8039, 0.8078, 0.4510],\n",
       "          [0.0000, 0.4784, 0.8588, 0.7569, 0.7020, 0.6706, 0.7176, 0.7686,\n",
       "           0.8000, 0.8235, 0.8353, 0.8118, 0.8275, 0.8235, 0.7843, 0.7686,\n",
       "           0.7608, 0.7490, 0.7647, 0.7490, 0.7765, 0.7529, 0.6902, 0.6118,\n",
       "           0.6549, 0.6941, 0.8235, 0.3608],\n",
       "          [0.0000, 0.0000, 0.2902, 0.7412, 0.8314, 0.7490, 0.6863, 0.6745,\n",
       "           0.6863, 0.7098, 0.7255, 0.7373, 0.7412, 0.7373, 0.7569, 0.7765,\n",
       "           0.8000, 0.8196, 0.8235, 0.8235, 0.8275, 0.7373, 0.7373, 0.7608,\n",
       "           0.7529, 0.8471, 0.6667, 0.0000],\n",
       "          [0.0078, 0.0000, 0.0000, 0.0000, 0.2588, 0.7843, 0.8706, 0.9294,\n",
       "           0.9373, 0.9490, 0.9647, 0.9529, 0.9569, 0.8667, 0.8627, 0.7569,\n",
       "           0.7490, 0.7020, 0.7137, 0.7137, 0.7098, 0.6902, 0.6510, 0.6588,\n",
       "           0.3882, 0.2275, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1569,\n",
       "           0.2392, 0.1725, 0.2824, 0.1608, 0.1373, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000]]]),\n",
       " 9)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See first training sample\n",
    "image, label = train_data[0]\n",
    "image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a88024-1a02-4efa-bf8f-8240bb3c39e5",
   "metadata": {},
   "source": [
    "### 1.1 Input and output shapes of a computer vision model\n",
    "\n",
    "We've got a big tensor of values (the image) leading to a single value for the target (the label).\n",
    "\n",
    "Let's see the image shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a387bab3-f331-4087-baad-2df1ac9bb31e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What's the shape of the image?\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a8c786-6ebf-4e3d-aaeb-b4ed8416a6f4",
   "metadata": {},
   "source": [
    "The shape of the image tensor is `[1, 28, 28]` or more specifically:\n",
    "\n",
    "```\n",
    "[color_channels=1, height=28, width=28]\n",
    "```\n",
    "\n",
    "Having `color_channels=1` means the image is grayscale.\n",
    "\n",
    "![example input and output shapes of the fashionMNIST problem](resources/03-computer-vision-input-and-output-shapes.png)\n",
    "*Various problems will have various input and output shapes. But the premise reamins: encode data into numbers, build a model to find patterns in those numbers, convert those patterns into something meaningful.*\n",
    "\n",
    "If `color_channels=3`, the image comes in pixel values for red, green and blue (this is also known a the [RGB color model](https://en.wikipedia.org/wiki/RGB_color_model)).\n",
    "\n",
    "The order of our current tensor is often referred to as `CHW` (Color Channels, Height, Width).\n",
    "\n",
    "There's debate on whether images should be represented as `CHW` (color channels first) or `HWC` (color channels last).\n",
    "\n",
    "> **Note:** You'll also see `NCHW` and `NHWC` formats where `N` stands for *number of images*. For example if you have a `batch_size=32`, your tensor shape may be `[32, 1, 28, 28]`. We'll cover batch sizes later.\n",
    "\n",
    "PyTorch generally accepts `NCHW` (channels first) as the default for many operators.\n",
    "\n",
    "However, PyTorch also explains that `NHWC` (channels last) performs better and is [considered best practice](https://pytorch.org/blog/tensor-memory-format-matters/#pytorch-best-practice). \n",
    "\n",
    "For now, since our dataset and models are relatively small, this won't make too much of a difference.\n",
    "\n",
    "But keep it in mind for when you're working on larger image datasets and using convolutional neural networks (we'll see these later).\n",
    "\n",
    "Let's check out more shapes of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8de326d4-03d1-4e75-af12-e318c61d0e83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 60000, 10000, 10000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many samples are there? \n",
    "len(train_data.data), len(train_data.targets), len(test_data.data), len(test_data.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8006cba4-b522-4ada-886e-d08b33016096",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T-shirt/top',\n",
       " 'Trouser',\n",
       " 'Pullover',\n",
       " 'Dress',\n",
       " 'Coat',\n",
       " 'Sandal',\n",
       " 'Shirt',\n",
       " 'Sneaker',\n",
       " 'Bag',\n",
       " 'Ankle boot']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See classes\n",
    "class_names = train_data.classes\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88e26f5f-cbcd-4bc0-bde6-52c9f6e26ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: torch.Size([1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "image, label = train_data[0]\n",
    "print(f\"Image shape: {image.shape}\")\n",
    "plt.imshow(image.squeeze()) # image shape is [1, 28, 28] (colour channels, height, width)\n",
    "plt.title(label);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544c763a-d36e-4a96-a817-7b289d9c3093",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image.squeeze(), cmap=\"gray\")\n",
    "plt.title(class_names[label]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255fe7ce-bd2d-4e98-910f-eacae9b5e6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot more images\n",
    "torch.manual_seed(42)\n",
    "fig = plt.figure(figsize=(9, 9))\n",
    "rows, cols = 4, 4\n",
    "for i in range(1, rows * cols + 1):\n",
    "    random_idx = torch.randint(0, len(train_data), size=[1]).item()\n",
    "    img, label = train_data[random_idx]\n",
    "    fig.add_subplot(rows, cols, i)\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "    plt.title(class_names[label])\n",
    "    plt.axis(False);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380dbb96-0165-4a1d-85d8-f90274e12d6d",
   "metadata": {},
   "source": [
    "## 2. Prepare DataLoader\n",
    "\n",
    "Now we've got a dataset ready to go.\n",
    "\n",
    "The next step is to prepare it with a [`torch.utils.data.DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) or `DataLoader` for short.\n",
    "\n",
    "The `DataLoader` does what you think it might do.\n",
    "\n",
    "It helps load data into a model.\n",
    "\n",
    "For training and for inference.\n",
    "\n",
    "It turns a large `Dataset` into a Python iterable of smaller chunks.\n",
    "\n",
    "These smaller chunks are called **batches** or **mini-batches** and can be set by the `batch_size` parameter.\n",
    "\n",
    "Why do this?\n",
    "\n",
    "Because it's more computationally efficient.\n",
    "\n",
    "In an ideal world you could do the forward pass and backward pass across all of your data at once.\n",
    "\n",
    "But once you start using really large datasets, unless you've got infinite computing power, it's easier to break them up into batches.\n",
    "\n",
    "It also gives your model more opportunities to improve.\n",
    "\n",
    "With **mini-batches** (small portions of the data), gradient descent is performed more often per epoch (once per mini-batch rather than once per epoch).\n",
    "\n",
    "What's a good batch size?\n",
    "\n",
    "[32 is a good place to start](https://twitter.com/ylecun/status/989610208497360896?s=20&t=N96J_jotN--PYuJk2WcjMw) for a fair amount of problems.\n",
    "\n",
    "But since this is a value you can set (a **hyperparameter**) you can try all different kinds of values, though generally powers of 2 are used most often (e.g. 32, 64, 128, 256, 512).\n",
    "\n",
    "![an example of what a batched dataset looks like](resources/03-batching-fashionmnist.png)\n",
    "*Batching FashionMNIST with a batch size of 32 and shuffle turned on. A similar batching process will occur for other datasets but will differ depending on the batch size.*\n",
    "\n",
    "Let's create `DataLoader`'s for our training and test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3fcd98-7e1c-4824-a5d8-be004917db7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Setup the batch size hyperparameter\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Turn datasets into iterables (batches)\n",
    "train_dataloader = DataLoader(train_data, # dataset to turn into iterable\n",
    "    batch_size=BATCH_SIZE, # how many samples per batch? \n",
    "    shuffle=True # shuffle data every epoch?\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(test_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False # don't necessarily have to shuffle the testing data\n",
    ")\n",
    "\n",
    "# Let's check out what we've created\n",
    "print(f\"Dataloaders: {train_dataloader, test_dataloader}\") \n",
    "print(f\"Length of train dataloader: {len(train_dataloader)} batches of {BATCH_SIZE}\")\n",
    "print(f\"Length of test dataloader: {len(test_dataloader)} batches of {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52373288-de4c-4c1f-9b47-e797f6fdf57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out what's inside the training dataloader\n",
    "train_features_batch, train_labels_batch = next(iter(train_dataloader))\n",
    "train_features_batch.shape, train_labels_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d147ec55-c354-4786-a0b3-220462b2be5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a sample\n",
    "torch.manual_seed(42)\n",
    "random_idx = torch.randint(0, len(train_features_batch), size=[1]).item()\n",
    "img, label = train_features_batch[random_idx], train_labels_batch[random_idx]\n",
    "plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.title(class_names[label])\n",
    "plt.axis(\"Off\");\n",
    "print(f\"Image size: {img.shape}\")\n",
    "print(f\"Label: {label}, label size: {label.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ffe878-5e65-4867-98eb-6e24d986fb7c",
   "metadata": {},
   "source": [
    "## 3. Model 0: Build a baseline model\n",
    "\n",
    "Data loaded and prepared!\n",
    "\n",
    "Time to build a **baseline model** by subclassing `nn.Module`.\n",
    "\n",
    "A **baseline model** is one of the simplest models you can imagine.\n",
    "\n",
    "You use the baseline as a starting point and try to improve upon it with subsequent, more complicated models.\n",
    "\n",
    "Our baseline will consist of two [`nn.Linear()`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) layers.\n",
    "\n",
    "We've done this in a previous section but there's going to one slight difference.\n",
    "\n",
    "Because we're working with image data, we're going to use a different layer to start things off.\n",
    "\n",
    "And that's the [`nn.Flatten()`](https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html) layer.\n",
    "\n",
    "`nn.Flatten()` compresses the dimensions of a tensor into a single vector.\n",
    "\n",
    "This is easier to understand when you see it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d205ec55-1790-433c-886c-4bd38f853d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a flatten layer\n",
    "flatten_model = nn.Flatten() # all nn modules function as a model (can do a forward pass)\n",
    "\n",
    "# Get a single sample\n",
    "x = train_features_batch[0]\n",
    "\n",
    "# Flatten the sample\n",
    "output = flatten_model(x) # perform forward pass\n",
    "\n",
    "# Print out what happened\n",
    "print(f\"Shape before flattening: {x.shape} -> [color_channels, height, width]\")\n",
    "print(f\"Shape after flattening: {output.shape} -> [color_channels, height*width]\")\n",
    "\n",
    "# Try uncommenting below and see what happens\n",
    "#print(x)\n",
    "#print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a4699b-00b2-4c5b-a7a4-953c0e622e67",
   "metadata": {},
   "source": [
    "The `nn.Flatten()` layer took our shape from `[color_channels, height, width]` to `[color_channels, height*width]`.\n",
    "\n",
    "Why do this?\n",
    "\n",
    "Because we've now turned our pixel data from height and width dimensions into one long **feature vector**.\n",
    "\n",
    "And `nn.Linear()` layers like their inputs to be in the form of feature vectors.\n",
    "\n",
    "Let's create our first model using `nn.Flatten()` as the first layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f27841a2-c7ea-4760-b712-fdce790a26b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class FashionMNISTModelV0(nn.Module):\n",
    "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
    "        super().__init__()\n",
    "        self.layer_stack = nn.Sequential(\n",
    "            nn.Flatten(), # neural networks like their inputs in vector form\n",
    "            nn.Linear(in_features=input_shape, out_features=hidden_units), # in_features = number of features in a data sample (784 pixels)\n",
    "            nn.Linear(in_features=hidden_units, out_features=output_shape)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layer_stack(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3c9f55-08a6-4a5a-abda-6729a85ca57d",
   "metadata": {},
   "source": [
    "We've got a baseline model class we can use, now let's instantiate a model.\n",
    "\n",
    "We'll need to set the following parameters:\n",
    "* `input_shape=784` - this is how many features you've got going in the model, in our case, it's one for every pixel in the target image (28 pixels high by 28 pixels wide = 784 features).\n",
    "* `hidden_units=10` - number of units/neurons in the hidden layer(s), this number could be whatever you want but to keep the model small we'll start with `10`.\n",
    "* `output_shape=len(class_names)` - since we're working with a multi-class classification problem, we need an output neuron per class in our dataset.\n",
    "\n",
    "Let's create an instance of our model and send to the CPU for now (we'll run a small test for running `model_0` on CPU vs. a similar model on GPU soon)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "868d4e4f-0348-445a-9e02-f99f50e6704a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Need to setup model with input parameters\u001b[39;00m\n\u001b[0;32m      4\u001b[0m model_0 \u001b[38;5;241m=\u001b[39m FashionMNISTModelV0(input_shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m784\u001b[39m, \u001b[38;5;66;03m# one for every pixel (28x28)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     hidden_units\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, \u001b[38;5;66;03m# how many units in the hiden layer\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     output_shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(class_names) \u001b[38;5;66;03m# one for every class\u001b[39;00m\n\u001b[0;32m      7\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# Need to setup model with input parameters\n",
    "model_0 = FashionMNISTModelV0(input_shape=784, # one for every pixel (28x28)\n",
    "    hidden_units=10, # how many units in the hiden layer\n",
    "    output_shape=len(class_names) # one for every class\n",
    ")\n",
    "model_0.to(\"cpu\") # keep model on CPU to begin with "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3336028d-eb13-4b9e-9efe-6b0954bf9829",
   "metadata": {},
   "source": [
    "### 3.1 Setup loss, optimizer and evaluation metrics\n",
    "\n",
    "> **Note:** Rather than importing and using our own accuracy function or evaluation metric(s), you could import various evaluation metrics from the [TorchMetrics package](https://torchmetrics.readthedocs.io/en/latest/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c155650-4aff-4033-9ffe-f48b3b205e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import accuracy metric\n",
    "from helper_functions import accuracy_fn # Note: could also use torchmetrics.Accuracy()\n",
    "\n",
    "# Setup loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss() # this is also called \"criterion\"/\"cost function\" in some places\n",
    "optimizer = torch.optim.SGD(params=model_0.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e77f31c-2971-4097-a322-1555bebb9b21",
   "metadata": {},
   "source": [
    "### 3.2 Creating a function to time our experiments\n",
    "\n",
    "Loss function and optimizer ready!\n",
    "\n",
    "It's time to start training a model.\n",
    "\n",
    "But how about we do a little experiment while we train.\n",
    "\n",
    "I mean, let's make a timing function to measure the time it takes our model to train on CPU versus using a GPU.\n",
    "\n",
    "We'll train this model on the CPU but the next one on the GPU and see what happens.\n",
    "\n",
    "Our timing function will import the [`timeit.default_timer()` function](https://docs.python.org/3/library/timeit.html#timeit.default_timer) from the Python [`timeit` module](https://docs.python.org/3/library/timeit.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "80f65093-83b3-4b47-becb-533a11a922c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer \n",
    "def print_train_time(start: float, end: float, device: torch.device = None):\n",
    "    \"\"\"Prints difference between start and end time.\n",
    "\n",
    "    Args:\n",
    "        start (float): Start time of computation (preferred in timeit format). \n",
    "        end (float): End time of computation.\n",
    "        device ([type], optional): Device that compute is running on. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        float: time between start and end in seconds (higher is longer).\n",
    "    \"\"\"\n",
    "    total_time = end - start\n",
    "    print(f\"Train time on {device}: {total_time:.3f} seconds\")\n",
    "    return total_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47a92ad-837d-4c60-b2a9-8e007c9a5f81",
   "metadata": {},
   "source": [
    "### 3.3 Creating a training loop and training a model on batches of data\n",
    "\n",
    "Beautiful!\n",
    "\n",
    "Looks like we've got all of the pieces of the puzzle ready to go, a timer, a loss function, an optimizer, a model and most importantly, some data.\n",
    "\n",
    "Let's now create a training loop and a testing loop to train and evaluate our model.\n",
    "\n",
    "We'll be using the same steps as the previous notebook(s), though since our data is now in batch form, we'll add another loop to loop through our data batches.\n",
    "\n",
    "Our data batches are contained within our `DataLoader`s, `train_dataloader` and `test_dataloader` for the training and test data splits respectively.\n",
    "\n",
    "A batch is `BATCH_SIZE` samples of `X` (features) and `y` (labels), since we're using `BATCH_SIZE=32`, our batches have 32 samples of images and targets.\n",
    "\n",
    "And since we're computing on batches of data, our loss and evaluation metrics will be calculated **per batch** rather than across the whole dataset.\n",
    "\n",
    "This means we'll have to divide our loss and accuracy values by the number of batches in each dataset's respective dataloader. \n",
    "\n",
    "Let's step through it: \n",
    "1. Loop through epochs.\n",
    "2. Loop through training batches, perform training steps, calculate the train loss *per batch*.\n",
    "3. Loop through testing batches, perform testing steps, calculate the test loss *per batch*.\n",
    "4. Print out what's happening.\n",
    "5. Time it all (for fun).\n",
    "\n",
    "A fair few steps but...\n",
    "\n",
    "...if in doubt, code it out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0ce075cf-5828-4e97-b378-b41fd5e1cc85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc4c1d348525470e99cb9dbf94801cc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "-------\n",
      "Looked at 0/60000 samples\n",
      "Looked at 12800/60000 samples\n",
      "Looked at 25600/60000 samples\n",
      "Looked at 38400/60000 samples\n",
      "Looked at 51200/60000 samples\n",
      "\n",
      "Train loss: 0.59039 | Test loss: 0.50954, Test acc: 82.04%\n",
      "\n",
      "Epoch: 1\n",
      "-------\n",
      "Looked at 0/60000 samples\n",
      "Looked at 12800/60000 samples\n",
      "Looked at 25600/60000 samples\n",
      "Looked at 38400/60000 samples\n",
      "Looked at 51200/60000 samples\n",
      "\n",
      "Train loss: 0.47633 | Test loss: 0.47989, Test acc: 83.20%\n",
      "\n",
      "Epoch: 2\n",
      "-------\n",
      "Looked at 0/60000 samples\n",
      "Looked at 12800/60000 samples\n",
      "Looked at 25600/60000 samples\n",
      "Looked at 38400/60000 samples\n",
      "Looked at 51200/60000 samples\n",
      "\n",
      "Train loss: 0.45503 | Test loss: 0.47664, Test acc: 83.43%\n",
      "\n",
      "Train time on cpu: 27.762 seconds\n"
     ]
    }
   ],
   "source": [
    "# Import tqdm for progress bar\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Set the seed and start the timer\n",
    "torch.manual_seed(42)\n",
    "train_time_start_on_cpu = timer()\n",
    "\n",
    "# Set the number of epochs (we'll keep this small for faster training times)\n",
    "epochs = 3\n",
    "\n",
    "# Create training and testing loop\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print(f\"Epoch: {epoch}\\n-------\")\n",
    "    ### Training\n",
    "    train_loss = 0\n",
    "    # Add a loop to loop through training batches\n",
    "    for batch, (X, y) in enumerate(train_dataloader):\n",
    "        model_0.train() \n",
    "        # 1. Forward pass\n",
    "        y_pred = model_0(X)\n",
    "\n",
    "        # 2. Calculate loss (per batch)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss # accumulatively add up the loss per epoch \n",
    "\n",
    "        # 3. Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 4. Loss backward\n",
    "        loss.backward()\n",
    "\n",
    "        # 5. Optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print out how many samples have been seen\n",
    "        if batch % 400 == 0:\n",
    "            print(f\"Looked at {batch * len(X)}/{len(train_dataloader.dataset)} samples\")\n",
    "\n",
    "    # Divide total train loss by length of train dataloader (average loss per batch per epoch)\n",
    "    train_loss /= len(train_dataloader)\n",
    "    \n",
    "    ### Testing\n",
    "    # Setup variables for accumulatively adding up loss and accuracy \n",
    "    test_loss, test_acc = 0, 0 \n",
    "    model_0.eval()\n",
    "    with torch.inference_mode():\n",
    "        for X, y in test_dataloader:\n",
    "            # 1. Forward pass\n",
    "            test_pred = model_0(X)\n",
    "           \n",
    "            # 2. Calculate loss (accumatively)\n",
    "            test_loss += loss_fn(test_pred, y) # accumulatively add up the loss per epoch\n",
    "\n",
    "            # 3. Calculate accuracy (preds need to be same as y_true)\n",
    "            test_acc += accuracy_fn(y_true=y, y_pred=test_pred.argmax(dim=1))\n",
    "        \n",
    "        # Calculations on test metrics need to happen inside torch.inference_mode()\n",
    "        # Divide total test loss by length of test dataloader (per batch)\n",
    "        test_loss /= len(test_dataloader)\n",
    "\n",
    "        # Divide total accuracy by length of test dataloader (per batch)\n",
    "        test_acc /= len(test_dataloader)\n",
    "\n",
    "    ## Print out what's happening\n",
    "    print(f\"\\nTrain loss: {train_loss:.5f} | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\\n\")\n",
    "\n",
    "# Calculate training time      \n",
    "train_time_end_on_cpu = timer()\n",
    "total_train_time_model_0 = print_train_time(start=train_time_start_on_cpu, \n",
    "                                           end=train_time_end_on_cpu,\n",
    "                                           device=str(next(model_0.parameters()).device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be3e84c-e821-4508-94a3-dbb714473bdc",
   "metadata": {},
   "source": [
    "## 4. Make predictions and get Model 0 results\n",
    "\n",
    "Since we're going to be building a few models, it's a good idea to write some code to evaluate them all in similar ways.\n",
    "\n",
    "Namely, let's create a function that takes in a trained model, a `DataLoader`, a loss function and an accuracy function.\n",
    "\n",
    "The function will use the model to make predictions on the data in the `DataLoader` and then we can evaluate those predictions using the loss function and accuracy function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "48007480-1d72-4bd1-b529-f4be7204183c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'FashionMNISTModelV0',\n",
       " 'model_loss': 0.4766390025615692,\n",
       " 'model_acc': 83.42651757188499}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "def eval_model(model: torch.nn.Module, \n",
    "               data_loader: torch.utils.data.DataLoader, \n",
    "               loss_fn: torch.nn.Module, \n",
    "               accuracy_fn):\n",
    "    \"\"\"Returns a dictionary containing the results of model predicting on data_loader.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): A PyTorch model capable of making predictions on data_loader.\n",
    "        data_loader (torch.utils.data.DataLoader): The target dataset to predict on.\n",
    "        loss_fn (torch.nn.Module): The loss function of model.\n",
    "        accuracy_fn: An accuracy function to compare the models predictions to the truth labels.\n",
    "\n",
    "    Returns:\n",
    "        (dict): Results of model making predictions on data_loader.\n",
    "    \"\"\"\n",
    "    loss, acc = 0, 0\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for X, y in data_loader:\n",
    "            # Make predictions with the model\n",
    "            y_pred = model(X)\n",
    "            \n",
    "            # Accumulate the loss and accuracy values per batch\n",
    "            loss += loss_fn(y_pred, y)\n",
    "            acc += accuracy_fn(y_true=y, \n",
    "                                y_pred=y_pred.argmax(dim=1)) # For accuracy, need the prediction labels (logits -> pred_prob -> pred_labels)\n",
    "        \n",
    "        # Scale loss and acc to find the average loss/acc per batch\n",
    "        loss /= len(data_loader)\n",
    "        acc /= len(data_loader)\n",
    "        \n",
    "    return {\"model_name\": model.__class__.__name__, # only works when model was created with a class\n",
    "            \"model_loss\": loss.item(),\n",
    "            \"model_acc\": acc}\n",
    "\n",
    "# Calculate model 0 results on test dataset\n",
    "model_0_results = eval_model(model=model_0, data_loader=test_dataloader,\n",
    "    loss_fn=loss_fn, accuracy_fn=accuracy_fn\n",
    ")\n",
    "model_0_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb91855-edcc-4764-b500-a8524e7ff14f",
   "metadata": {},
   "source": [
    "## 5. Setup device agnostic-code (for using a GPU if there is one)\n",
    "We've seen how long it takes to train ma PyTorch model on 60,000 samples on CPU.\n",
    "\n",
    "> **Note:** Model training time is dependent on hardware used. Generally, more processors means faster training and smaller models on smaller datasets will often train faster than large models and large datasets.\n",
    "\n",
    "Now let's setup some [device-agnostic code](https://pytorch.org/docs/stable/notes/cuda.html#best-practices) for our models and data to run on GPU if it's available.\n",
    "\n",
    "If you're running this notebook on Google Colab, and you don't a GPU turned on yet, it's now time to turn one on via `Runtime -> Change runtime type -> Hardware accelerator -> GPU`. If you do this, your runtime will likely reset and you'll have to run all of the cells above by going `Runtime -> Run before`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0440781f-36d3-46c3-92bb-7510d3a06ff3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup device agnostic code\n",
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8671b27c-c636-4bc3-a6d9-20c79b0e884d",
   "metadata": {},
   "source": [
    "## 6. Model 1: Building a better model with non-linearity\n",
    "\n",
    "Seeing the data we've been working with, do you think it needs non-linear functions?\n",
    "\n",
    "And remember, linear means straight and non-linear means non-straight.\n",
    "\n",
    "Let's find out.\n",
    "\n",
    "We'll do so by recreating a similar model to before, except this time we'll put non-linear functions (`nn.ReLU()`) in between each linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bcb4e1-511e-4a40-967e-f37a8bd73ee3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
