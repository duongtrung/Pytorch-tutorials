{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/duongtrung/Pytorch-tutorials/blob/main/13_pytorch_reinforcement_learning_TD3_Half_Cheetah_up.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Paper](http://proceedings.mlr.press/v80/fujimoto18a/fujimoto18a.pdf): Addressing Function Approximation Error in Actor-Critic Methods, ICML2018\n",
    "\n",
    "Open AI [Tutorial](https://spinningup.openai.com/en/latest/algorithms/td3.html)\n",
    "\n",
    "Official [Git](https://github.com/sfujim/TD3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WXu1r8qvSzWf"
   },
   "source": [
    "# Twin-Delayed DDPG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YRzQUhuUTc0J"
   },
   "source": [
    "## Installing the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HAHMB0Ze8fU0"
   },
   "outputs": [],
   "source": [
    "# !pip install pybullet\n",
    "# conda install -c conda-forge pybullet\n",
    "# conda install -c conda-forge gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xjm2onHdT-Av"
   },
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ikr2p0Js8iB4"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pybullet_envs\n",
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from gym import wrappers\n",
    "from torch.autograd import Variable\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y2nGdtlKVydr"
   },
   "source": [
    "## Step 1: We initialize the Experience Replay memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u5rW0IDB8nTO"
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    '''\n",
    "    Memory object\n",
    "    '''\n",
    "\n",
    "    def __init__(self, max_size=1e6): # maximum transitions that it can store, let's say 1 million\n",
    "        self.storage = []\n",
    "        self.max_size = max_size\n",
    "        self.ptr = 0\n",
    "\n",
    "    def add(self, transition):\n",
    "        if len(self.storage) == self.max_size:\n",
    "            self.storage[int(self.ptr)] = transition\n",
    "            self.ptr = (self.ptr + 1) % self.max_size # go back to the beginning\n",
    "        else:\n",
    "            self.storage.append(transition)\n",
    "\n",
    "    def sample(self, batch_size): # get transitions in the memory and put them input a batch\n",
    "        ind = np.random.randint(0, len(self.storage), size=batch_size)\n",
    "        batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = [], [], [], [], []\n",
    "        for i in ind: \n",
    "            state, next_state, action, reward, done = self.storage[i]\n",
    "            batch_states.append(np.array(state, copy=False))  # make reference, not actually copy\n",
    "            batch_next_states.append(np.array(next_state, copy=False))\n",
    "            batch_actions.append(np.array(action, copy=False))\n",
    "            batch_rewards.append(np.array(reward, copy=False))\n",
    "            batch_dones.append(np.array(done, copy=False))\n",
    "        return np.array(batch_states), np.array(batch_next_states), np.array(batch_actions), np.array(batch_rewards).reshape(-1, 1), np.array(batch_dones).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jb7TTaHxWbQD"
   },
   "source": [
    "## Step 2: We build one neural network for the Actor model and one neural network for the Actor target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4CeRW4D79HL0"
   },
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "  \n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        super(Actor, self).__init__()\n",
    "        self.layer_1 = nn.Linear(state_dim, 400)\n",
    "        self.layer_2 = nn.Linear(400, 300)\n",
    "        self.layer_3 = nn.Linear(300, action_dim)\n",
    "        self.max_action = max_action\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = F.relu(self.layer_1(x))\n",
    "        #x = F.relu(self.layer_2(x))\n",
    "        #x = self.max_action * torch.tanh(self.layer_3(x))\n",
    "        x = self.max_action * torch.tanh(self.layer_3( F.relu(self.layer_2( F.relu(self.layer_1(x)) )) ))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HRDDce8FXef7"
   },
   "source": [
    "## Step 3: We build two neural networks for the two Critic models and two neural networks for the two Critic targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OCee7gwR9Jrs"
   },
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "  \n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        # Defining the first Critic neural network\n",
    "        self.layer_1 = nn.Linear(state_dim + action_dim, 400)\n",
    "        self.layer_2 = nn.Linear(400, 300)\n",
    "        self.layer_3 = nn.Linear(300, 1)\n",
    "        # Defining the second Critic neural network\n",
    "        self.layer_4 = nn.Linear(state_dim + action_dim, 400)\n",
    "        self.layer_5 = nn.Linear(400, 300)\n",
    "        self.layer_6 = nn.Linear(300, 1)\n",
    "\n",
    "    def forward(self, x, u):\n",
    "        xu = torch.cat([x, u], 1)\n",
    "        # Forward-Propagation on the first Critic Neural Network\n",
    "        #x1 = F.relu(self.layer_1(xu))\n",
    "        #x1 = F.relu(self.layer_2(x1))\n",
    "        #x1 = self.layer_3(x1)\n",
    "        x1 = self.layer_3( F.relu(self.layer_2( F.relu(self.layer_1(xu)) )) )\n",
    "        # Forward-Propagation on the second Critic Neural Network\n",
    "        #x2 = F.relu(self.layer_4(xu))\n",
    "        #x2 = F.relu(self.layer_5(x2))\n",
    "        #x2 = self.layer_6(x2)\n",
    "        x2 = self.layer_6( F.relu(self.layer_5( F.relu(self.layer_4(xu)) )) )\n",
    "        return x1, x2\n",
    "\n",
    "    def Q1(self, x, u):\n",
    "        #xu = torch.cat([x, u], 1)\n",
    "        #x1 = F.relu(self.layer_1(xu))\n",
    "        #x1 = F.relu(self.layer_2(x1))\n",
    "        #x1 = self.layer_3(x1)\n",
    "        x1 = self.layer_3( F.relu(self.layer_2( F.relu(self.layer_1( torch.cat([x, u], 1) )) )) )\n",
    "        return x1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NzIDuONodenW"
   },
   "source": [
    "## Steps 4 to 15: Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zzd0H1xukdKe"
   },
   "outputs": [],
   "source": [
    "# Selecting the device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Building the whole Training Process into a class\n",
    "\n",
    "class TD3(object):\n",
    "  \n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
    "        self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n",
    "        self.critic = Critic(state_dim, action_dim).to(device)\n",
    "        self.critic_target = Critic(state_dim, action_dim).to(device)\n",
    "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
    "        self.max_action = max_action\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.Tensor(state.reshape(1, -1)).to(device)\n",
    "        return self.actor(state).cpu().data.numpy().flatten()\n",
    "\n",
    "    def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
    "    \n",
    "        for it in range(iterations):\n",
    "      \n",
    "            # Step 4: We sample a batch of transitions (s, s’, a, r) from the memory\n",
    "            batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
    "            state = torch.Tensor(batch_states).to(device)\n",
    "            next_state = torch.Tensor(batch_next_states).to(device)\n",
    "            action = torch.Tensor(batch_actions).to(device)\n",
    "            reward = torch.Tensor(batch_rewards).to(device)\n",
    "            done = torch.Tensor(batch_dones).to(device)\n",
    "      \n",
    "            # Step 5: From the next state s’, the Actor target plays the next action a’\n",
    "            next_action = self.actor_target(next_state)\n",
    "      \n",
    "            # Step 6: We add Gaussian noise to this next action a’ and we clamp it in a range of values supported by the environment\n",
    "            noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)\n",
    "            noise = noise.clamp(-noise_clip, noise_clip)\n",
    "            next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n",
    "      \n",
    "            # Step 7: The two Critic targets take each the couple (s’, a’) as input and return two Q-values Qt1(s’,a’) and Qt2(s’,a’) as outputs\n",
    "            target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
    "      \n",
    "            # Step 8: We keep the minimum of these two Q-values: min(Qt1, Qt2)\n",
    "            target_Q = torch.min(target_Q1, target_Q2)\n",
    "      \n",
    "            # Step 9: We get the final target of the two Critic models, which is: Qt = r + γ * min(Qt1, Qt2), where γ is the discount factor\n",
    "            target_Q = reward + ((1 - done) * discount * target_Q).detach()\n",
    "      \n",
    "            # Step 10: The two Critic models take each the couple (s, a) as input and return two Q-values Q1(s,a) and Q2(s,a) as outputs\n",
    "            current_Q1, current_Q2 = self.critic(state, action)\n",
    "      \n",
    "            # Step 11: We compute the loss coming from the two Critic models: Critic Loss = MSE_Loss(Q1(s,a), Qt) + MSE_Loss(Q2(s,a), Qt)\n",
    "            critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
    "      \n",
    "            # Step 12: We backpropagate this Critic loss and update the parameters of the two Critic models with a SGD optimizer\n",
    "            self.critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            self.critic_optimizer.step()\n",
    "      \n",
    "            # Step 13: Once every two iterations, we update our Actor model by performing gradient ascent on the output of the first Critic model\n",
    "            if it % policy_freq == 0:\n",
    "                actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
    "                self.actor_optimizer.zero_grad()\n",
    "                actor_loss.backward()\n",
    "                self.actor_optimizer.step()\n",
    "        \n",
    "            # Step 14: Still once every two iterations, we update the weights of the Actor target by polyak averaging\n",
    "            for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "                target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "        \n",
    "            # Step 15: Still once every two iterations, we update the weights of the Critic target by polyak averaging\n",
    "                for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "                    target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "  \n",
    "    # Making a save method to save a trained model\n",
    "    def save(self, filename, directory):\n",
    "        torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
    "        torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
    "  \n",
    "    # Making a load method to load a pre-trained model\n",
    "    def load(self, filename, directory):\n",
    "        self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
    "        self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ka-ZRtQvjBex"
   },
   "source": [
    "## We make a function that evaluates the policy by calculating its average reward over 10 episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qabqiYdp9wDM"
   },
   "outputs": [],
   "source": [
    "def evaluate_policy(policy, eval_episodes=10):\n",
    "    avg_reward = 0.\n",
    "    for _ in range(eval_episodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = policy.select_action(np.array(obs))\n",
    "            obs, reward, done, _ = env.step(action)\n",
    "            avg_reward += reward\n",
    "  \n",
    "    avg_reward /= eval_episodes\n",
    "    print (\"---------------------------------------\")\n",
    "    print (\"Average Reward over the Evaluation Step: %f\" % (avg_reward))\n",
    "    print (\"---------------------------------------\")\n",
    "    return avg_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gGuKmH_ijf7U"
   },
   "source": [
    "## We set the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HFj6wbAo97lk"
   },
   "outputs": [],
   "source": [
    "env_name = \"HalfCheetahBulletEnv-v0\" # Name of a environment (set it to any Continous environment you want)\n",
    "seed = 0 # Random seed number\n",
    "start_timesteps = 1e4 # Number of iterations/timesteps before which the model randomly chooses an action, and after which it starts to use the policy network\n",
    "eval_freq = 5e3 # How often the evaluation step is performed (after how many timesteps)\n",
    "max_timesteps = 5e5 # Total number of iterations/timesteps\n",
    "save_models = True # Boolean checker whether or not to save the pre-trained model\n",
    "expl_noise = 0.1 # Exploration noise - STD value of exploration Gaussian noise\n",
    "batch_size = 100 # Size of the batch\n",
    "discount = 0.99 # Discount factor gamma, used in the calculation of the total discounted reward\n",
    "tau = 0.005 # Target network update rate\n",
    "policy_noise = 0.2 # STD of Gaussian noise added to the actions for the exploration purposes\n",
    "noise_clip = 0.5 # Maximum value of the Gaussian noise added to the actions (policy)\n",
    "policy_freq = 2 # Number of iterations to wait before the policy network (Actor model) is updated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hjwf2HCol3XP"
   },
   "source": [
    "## We create a file name for the two saved models: the Actor and Critic models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1fyH8N5z-o3o"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Settings: TD3_HalfCheetahBulletEnv-v0_0\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "file_name = \"%s_%s_%s\" % (\"TD3\", env_name, str(seed))\n",
    "print (\"---------------------------------------\")\n",
    "print (\"Settings: %s\" % (file_name))\n",
    "print (\"---------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kop-C96Aml8O"
   },
   "source": [
    "## We create a folder inside which will be saved the trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Src07lvY-zXb"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(\"TD3\"):\n",
    "    os.makedirs(\"TD3\")\n",
    "if save_models and not os.path.exists(\"TD3/pytorch_models\"):\n",
    "    os.makedirs(\"TD3/pytorch_models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qEAzOd47mv1Z"
   },
   "source": [
    "## We create the PyBullet environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CyQXJUIs-6BV"
   },
   "outputs": [],
   "source": [
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5YdPG4HXnNsh"
   },
   "source": [
    "## We set seeds and we get the necessary information on the states and actions in the chosen environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z3RufYec_ADj"
   },
   "outputs": [],
   "source": [
    "env.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "max_action = float(env.action_space.high[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HWEgDAQxnbem"
   },
   "source": [
    "## We create the policy network (the Actor model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wTVvG7F8_EWg"
   },
   "outputs": [],
   "source": [
    "policy = TD3(state_dim, action_dim, max_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZI60VN2Unklh"
   },
   "source": [
    "## We create the Experience Replay memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sd-ZsdXR_LgV"
   },
   "outputs": [],
   "source": [
    "replay_buffer = ReplayBuffer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QYOpCyiDnw7s"
   },
   "source": [
    "## We define a list where all the evaluation results over 10 episodes are stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dhC_5XJ__Orp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1429.426658\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "evaluations = [evaluate_policy(policy)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xm-4b3p6rglE"
   },
   "source": [
    "## We create a new folder directory in which the final results (videos of the agent) will be populated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MTL9uMd0ru03"
   },
   "outputs": [],
   "source": [
    "def mkdir(base, name):\n",
    "    path = os.path.join(base, name)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    return path\n",
    "work_dir = mkdir('exp', 'brs')\n",
    "monitor_dir = mkdir(work_dir, 'monitor')\n",
    "max_episode_steps = env._max_episode_steps\n",
    "save_env_vid = False\n",
    "if save_env_vid:\n",
    "    env = wrappers.Monitor(env, monitor_dir, force = True)\n",
    "    env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "31n5eb03p-Fm"
   },
   "source": [
    "## We initialize the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1vN5EvxK_QhT"
   },
   "outputs": [],
   "source": [
    "total_timesteps = 0\n",
    "timesteps_since_eval = 0\n",
    "episode_num = 0\n",
    "done = True\n",
    "t0 = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q9gsjvtPqLgT"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y_ouY4NH_Y0I"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Timesteps: 1000 Episode Num: 1 Reward: -1346.0160848338146\n",
      "Total Timesteps: 2000 Episode Num: 2 Reward: -1168.1591544020462\n",
      "Total Timesteps: 3000 Episode Num: 3 Reward: -1135.6267678399322\n",
      "Total Timesteps: 4000 Episode Num: 4 Reward: -1144.5003405745329\n",
      "Total Timesteps: 5000 Episode Num: 5 Reward: -1313.9147251020402\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1548.323476\n",
      "---------------------------------------\n",
      "Total Timesteps: 6000 Episode Num: 6 Reward: -1228.8228953972693\n",
      "Total Timesteps: 7000 Episode Num: 7 Reward: -1260.0969081678643\n",
      "Total Timesteps: 8000 Episode Num: 8 Reward: -1327.9898236234571\n",
      "Total Timesteps: 9000 Episode Num: 9 Reward: -1218.8606909043501\n",
      "Total Timesteps: 10000 Episode Num: 10 Reward: -1344.4311646204771\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1641.232559\n",
      "---------------------------------------\n",
      "Total Timesteps: 11000 Episode Num: 11 Reward: -1661.2168760923864\n",
      "Total Timesteps: 12000 Episode Num: 12 Reward: -1442.5244912995831\n",
      "Total Timesteps: 13000 Episode Num: 13 Reward: -1556.7619839727502\n",
      "Total Timesteps: 14000 Episode Num: 14 Reward: -1490.8893487165085\n",
      "Total Timesteps: 15000 Episode Num: 15 Reward: 500.13050495884875\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1467.781176\n",
      "---------------------------------------\n",
      "Total Timesteps: 16000 Episode Num: 16 Reward: -1479.3744805850693\n",
      "Total Timesteps: 17000 Episode Num: 17 Reward: -1486.1745975001306\n",
      "Total Timesteps: 18000 Episode Num: 18 Reward: -1475.327081162281\n",
      "Total Timesteps: 19000 Episode Num: 19 Reward: -1470.1668628320663\n",
      "Total Timesteps: 20000 Episode Num: 20 Reward: -1377.8717655631247\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1519.365769\n",
      "---------------------------------------\n",
      "Total Timesteps: 21000 Episode Num: 21 Reward: -1441.0110829938546\n",
      "Total Timesteps: 22000 Episode Num: 22 Reward: 363.91793360577293\n",
      "Total Timesteps: 23000 Episode Num: 23 Reward: -799.7821978648137\n",
      "Total Timesteps: 24000 Episode Num: 24 Reward: -1440.6531537369424\n",
      "Total Timesteps: 25000 Episode Num: 25 Reward: -1389.066775058902\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -800.769013\n",
      "---------------------------------------\n",
      "Total Timesteps: 26000 Episode Num: 26 Reward: -1386.5222016321316\n",
      "Total Timesteps: 27000 Episode Num: 27 Reward: 223.69054313204586\n",
      "Total Timesteps: 28000 Episode Num: 28 Reward: 656.9209428408133\n",
      "Total Timesteps: 29000 Episode Num: 29 Reward: 401.78670949380796\n",
      "Total Timesteps: 30000 Episode Num: 30 Reward: -352.96057230349885\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 335.319393\n",
      "---------------------------------------\n",
      "Total Timesteps: 31000 Episode Num: 31 Reward: 725.8841652934442\n",
      "Total Timesteps: 32000 Episode Num: 32 Reward: 542.1596703900409\n",
      "Total Timesteps: 33000 Episode Num: 33 Reward: -901.0467188811633\n",
      "Total Timesteps: 34000 Episode Num: 34 Reward: 711.9744397994882\n",
      "Total Timesteps: 35000 Episode Num: 35 Reward: 58.67885606991111\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 650.424782\n",
      "---------------------------------------\n",
      "Total Timesteps: 36000 Episode Num: 36 Reward: 668.7044866965311\n",
      "Total Timesteps: 37000 Episode Num: 37 Reward: 586.7030061826281\n",
      "Total Timesteps: 38000 Episode Num: 38 Reward: 653.6076887883711\n",
      "Total Timesteps: 39000 Episode Num: 39 Reward: 535.8997818428985\n",
      "Total Timesteps: 40000 Episode Num: 40 Reward: -74.66915702228631\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 580.754358\n",
      "---------------------------------------\n",
      "Total Timesteps: 41000 Episode Num: 41 Reward: 653.1014101504296\n",
      "Total Timesteps: 42000 Episode Num: 42 Reward: 698.5324534894369\n",
      "Total Timesteps: 43000 Episode Num: 43 Reward: 517.6251886205031\n",
      "Total Timesteps: 44000 Episode Num: 44 Reward: 614.5171019281112\n",
      "Total Timesteps: 45000 Episode Num: 45 Reward: 486.11599328880726\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 542.980850\n",
      "---------------------------------------\n",
      "Total Timesteps: 46000 Episode Num: 46 Reward: 519.4622369998372\n",
      "Total Timesteps: 47000 Episode Num: 47 Reward: 658.656774983633\n",
      "Total Timesteps: 48000 Episode Num: 48 Reward: 549.2905265524186\n",
      "Total Timesteps: 49000 Episode Num: 49 Reward: 577.2276118171466\n",
      "Total Timesteps: 50000 Episode Num: 50 Reward: 690.8388302255597\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 796.679045\n",
      "---------------------------------------\n",
      "Total Timesteps: 51000 Episode Num: 51 Reward: 791.5020143108411\n",
      "Total Timesteps: 52000 Episode Num: 52 Reward: 760.7445223115559\n",
      "Total Timesteps: 53000 Episode Num: 53 Reward: 752.4061444433152\n",
      "Total Timesteps: 54000 Episode Num: 54 Reward: 633.9389085350223\n",
      "Total Timesteps: 55000 Episode Num: 55 Reward: 634.019969191863\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 473.967019\n",
      "---------------------------------------\n",
      "Total Timesteps: 56000 Episode Num: 56 Reward: 321.035113816282\n",
      "Total Timesteps: 57000 Episode Num: 57 Reward: 531.9142650797659\n",
      "Total Timesteps: 58000 Episode Num: 58 Reward: 516.8650378454643\n",
      "Total Timesteps: 59000 Episode Num: 59 Reward: 664.4295756387105\n",
      "Total Timesteps: 60000 Episode Num: 60 Reward: 668.4160062352393\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 129.574569\n",
      "---------------------------------------\n",
      "Total Timesteps: 61000 Episode Num: 61 Reward: 667.8766960220569\n",
      "Total Timesteps: 62000 Episode Num: 62 Reward: 664.4563284503209\n",
      "Total Timesteps: 63000 Episode Num: 63 Reward: 502.5826445751615\n",
      "Total Timesteps: 64000 Episode Num: 64 Reward: 679.185651512646\n",
      "Total Timesteps: 65000 Episode Num: 65 Reward: -158.4902413980006\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 66.644153\n",
      "---------------------------------------\n",
      "Total Timesteps: 66000 Episode Num: 66 Reward: 529.609365301652\n",
      "Total Timesteps: 67000 Episode Num: 67 Reward: 675.4464460936667\n",
      "Total Timesteps: 68000 Episode Num: 68 Reward: 393.42821936285003\n",
      "Total Timesteps: 69000 Episode Num: 69 Reward: 659.1423358314793\n",
      "Total Timesteps: 70000 Episode Num: 70 Reward: 697.084053893167\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 140.809512\n",
      "---------------------------------------\n",
      "Total Timesteps: 71000 Episode Num: 71 Reward: 704.2139163536361\n",
      "Total Timesteps: 72000 Episode Num: 72 Reward: 656.5404713960194\n",
      "Total Timesteps: 73000 Episode Num: 73 Reward: 553.2031406680621\n",
      "Total Timesteps: 74000 Episode Num: 74 Reward: 453.01746983059724\n",
      "Total Timesteps: 75000 Episode Num: 75 Reward: 748.7480308620543\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 723.699897\n",
      "---------------------------------------\n",
      "Total Timesteps: 76000 Episode Num: 76 Reward: 793.1069050607304\n",
      "Total Timesteps: 77000 Episode Num: 77 Reward: 772.4259232747513\n",
      "Total Timesteps: 78000 Episode Num: 78 Reward: 604.7196509072498\n",
      "Total Timesteps: 79000 Episode Num: 79 Reward: 521.9432570540145\n",
      "Total Timesteps: 80000 Episode Num: 80 Reward: 666.5686090373343\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 537.102673\n",
      "---------------------------------------\n",
      "Total Timesteps: 81000 Episode Num: 81 Reward: 533.222170776432\n",
      "Total Timesteps: 82000 Episode Num: 82 Reward: 546.4406298385867\n",
      "Total Timesteps: 83000 Episode Num: 83 Reward: 734.6637218662237\n",
      "Total Timesteps: 84000 Episode Num: 84 Reward: 687.4170814253467\n",
      "Total Timesteps: 85000 Episode Num: 85 Reward: 713.4352891225209\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 657.337721\n",
      "---------------------------------------\n",
      "Total Timesteps: 86000 Episode Num: 86 Reward: 614.9577633406844\n",
      "Total Timesteps: 87000 Episode Num: 87 Reward: 603.6789018778419\n",
      "Total Timesteps: 88000 Episode Num: 88 Reward: 721.2804498834699\n",
      "Total Timesteps: 89000 Episode Num: 89 Reward: 530.1146505721274\n",
      "Total Timesteps: 90000 Episode Num: 90 Reward: 437.45443212558274\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 679.855703\n",
      "---------------------------------------\n",
      "Total Timesteps: 91000 Episode Num: 91 Reward: 787.042785154209\n",
      "Total Timesteps: 92000 Episode Num: 92 Reward: 448.3279009034036\n",
      "Total Timesteps: 93000 Episode Num: 93 Reward: 741.6800313252444\n",
      "Total Timesteps: 94000 Episode Num: 94 Reward: 724.2417459277376\n",
      "Total Timesteps: 95000 Episode Num: 95 Reward: 591.2447252387996\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 493.596072\n",
      "---------------------------------------\n",
      "Total Timesteps: 96000 Episode Num: 96 Reward: 580.8746468269472\n",
      "Total Timesteps: 97000 Episode Num: 97 Reward: 643.7437484441657\n",
      "Total Timesteps: 98000 Episode Num: 98 Reward: 479.1581479020977\n",
      "Total Timesteps: 99000 Episode Num: 99 Reward: -10.99815649003912\n",
      "Total Timesteps: 100000 Episode Num: 100 Reward: 589.895590948393\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 626.107722\n",
      "---------------------------------------\n",
      "Total Timesteps: 101000 Episode Num: 101 Reward: 648.5340437352397\n",
      "Total Timesteps: 102000 Episode Num: 102 Reward: 665.843972278058\n",
      "Total Timesteps: 103000 Episode Num: 103 Reward: 756.3827681935131\n",
      "Total Timesteps: 104000 Episode Num: 104 Reward: 723.0632626869113\n",
      "Total Timesteps: 105000 Episode Num: 105 Reward: 652.5947664943728\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 660.492202\n",
      "---------------------------------------\n",
      "Total Timesteps: 106000 Episode Num: 106 Reward: 650.745414252822\n",
      "Total Timesteps: 107000 Episode Num: 107 Reward: 679.105060645812\n",
      "Total Timesteps: 108000 Episode Num: 108 Reward: 672.4808514329286\n",
      "Total Timesteps: 109000 Episode Num: 109 Reward: 641.021660766574\n",
      "Total Timesteps: 110000 Episode Num: 110 Reward: 753.0882603916959\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 661.831974\n",
      "---------------------------------------\n",
      "Total Timesteps: 111000 Episode Num: 111 Reward: 672.9804177038212\n",
      "Total Timesteps: 112000 Episode Num: 112 Reward: 664.3013114245081\n",
      "Total Timesteps: 113000 Episode Num: 113 Reward: 766.3493897906703\n",
      "Total Timesteps: 114000 Episode Num: 114 Reward: 654.5683030672853\n",
      "Total Timesteps: 115000 Episode Num: 115 Reward: 653.5863201065267\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 729.004709\n",
      "---------------------------------------\n",
      "Total Timesteps: 116000 Episode Num: 116 Reward: 671.8471227920825\n",
      "Total Timesteps: 117000 Episode Num: 117 Reward: 728.5339758012105\n",
      "Total Timesteps: 118000 Episode Num: 118 Reward: 550.2085202118782\n",
      "Total Timesteps: 119000 Episode Num: 119 Reward: 747.0812059413762\n",
      "Total Timesteps: 120000 Episode Num: 120 Reward: 581.3941471932351\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 670.274184\n",
      "---------------------------------------\n",
      "Total Timesteps: 121000 Episode Num: 121 Reward: 690.90343830961\n",
      "Total Timesteps: 122000 Episode Num: 122 Reward: 721.6583454115303\n",
      "Total Timesteps: 123000 Episode Num: 123 Reward: 642.8687070940505\n",
      "Total Timesteps: 124000 Episode Num: 124 Reward: 688.1561193024256\n",
      "Total Timesteps: 125000 Episode Num: 125 Reward: 730.8835881031953\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 673.989691\n",
      "---------------------------------------\n",
      "Total Timesteps: 126000 Episode Num: 126 Reward: 645.38893093558\n",
      "Total Timesteps: 127000 Episode Num: 127 Reward: 728.0506895785719\n",
      "Total Timesteps: 128000 Episode Num: 128 Reward: 690.1043177752591\n",
      "Total Timesteps: 129000 Episode Num: 129 Reward: 571.2197506374549\n",
      "Total Timesteps: 130000 Episode Num: 130 Reward: 654.8445413054374\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 722.489007\n",
      "---------------------------------------\n",
      "Total Timesteps: 131000 Episode Num: 131 Reward: 683.5362624798474\n",
      "Total Timesteps: 132000 Episode Num: 132 Reward: 663.6638399148625\n",
      "Total Timesteps: 133000 Episode Num: 133 Reward: 670.4573650217169\n",
      "Total Timesteps: 134000 Episode Num: 134 Reward: 717.4455536957777\n",
      "Total Timesteps: 135000 Episode Num: 135 Reward: 730.6948290073402\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 859.576059\n",
      "---------------------------------------\n",
      "Total Timesteps: 136000 Episode Num: 136 Reward: 789.4965382788527\n",
      "Total Timesteps: 137000 Episode Num: 137 Reward: 409.0695437196648\n",
      "Total Timesteps: 138000 Episode Num: 138 Reward: 741.9616955664628\n",
      "Total Timesteps: 139000 Episode Num: 139 Reward: 852.1915139768533\n",
      "Total Timesteps: 140000 Episode Num: 140 Reward: 857.9057243370593\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 874.256381\n",
      "---------------------------------------\n",
      "Total Timesteps: 141000 Episode Num: 141 Reward: 856.3250780756753\n",
      "Total Timesteps: 142000 Episode Num: 142 Reward: 769.3608603703742\n",
      "Total Timesteps: 143000 Episode Num: 143 Reward: 680.77660521831\n",
      "Total Timesteps: 144000 Episode Num: 144 Reward: 734.4288030681453\n",
      "Total Timesteps: 145000 Episode Num: 145 Reward: 862.569072590496\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 482.313810\n",
      "---------------------------------------\n",
      "Total Timesteps: 146000 Episode Num: 146 Reward: 811.638500877546\n",
      "Total Timesteps: 147000 Episode Num: 147 Reward: 647.6631765487751\n",
      "Total Timesteps: 148000 Episode Num: 148 Reward: 617.1345075575093\n",
      "Total Timesteps: 149000 Episode Num: 149 Reward: 749.3475561673324\n",
      "Total Timesteps: 150000 Episode Num: 150 Reward: 755.6305793068465\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 849.282634\n",
      "---------------------------------------\n",
      "Total Timesteps: 151000 Episode Num: 151 Reward: 816.4174173438134\n",
      "Total Timesteps: 152000 Episode Num: 152 Reward: 815.0683542742864\n",
      "Total Timesteps: 153000 Episode Num: 153 Reward: 793.9173772451223\n",
      "Total Timesteps: 154000 Episode Num: 154 Reward: 785.3389154616962\n",
      "Total Timesteps: 155000 Episode Num: 155 Reward: 784.5764148478113\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 872.591179\n",
      "---------------------------------------\n",
      "Total Timesteps: 156000 Episode Num: 156 Reward: 851.723491173571\n",
      "Total Timesteps: 157000 Episode Num: 157 Reward: 752.3899365443465\n",
      "Total Timesteps: 158000 Episode Num: 158 Reward: -1559.8728035939746\n",
      "Total Timesteps: 159000 Episode Num: 159 Reward: -1589.9520846893026\n",
      "Total Timesteps: 160000 Episode Num: 160 Reward: -1697.8366668732886\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1692.746168\n",
      "---------------------------------------\n",
      "Total Timesteps: 161000 Episode Num: 161 Reward: -1695.5311170768493\n",
      "Total Timesteps: 162000 Episode Num: 162 Reward: -1719.2169764462346\n",
      "Total Timesteps: 163000 Episode Num: 163 Reward: 101.09239282936348\n",
      "Total Timesteps: 164000 Episode Num: 164 Reward: 357.2801080964216\n",
      "Total Timesteps: 165000 Episode Num: 165 Reward: 454.8309890252199\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 514.319418\n",
      "---------------------------------------\n",
      "Total Timesteps: 166000 Episode Num: 166 Reward: 528.7712757719567\n",
      "Total Timesteps: 167000 Episode Num: 167 Reward: 190.81763071116217\n",
      "Total Timesteps: 168000 Episode Num: 168 Reward: 46.269424539098615\n",
      "Total Timesteps: 169000 Episode Num: 169 Reward: -244.51749261151994\n",
      "Total Timesteps: 170000 Episode Num: 170 Reward: 647.3598514917571\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 838.537072\n",
      "---------------------------------------\n",
      "Total Timesteps: 171000 Episode Num: 171 Reward: 826.1220397516362\n",
      "Total Timesteps: 172000 Episode Num: 172 Reward: 753.0650549038974\n",
      "Total Timesteps: 173000 Episode Num: 173 Reward: 560.1210102396395\n",
      "Total Timesteps: 174000 Episode Num: 174 Reward: 404.5869043984392\n",
      "Total Timesteps: 175000 Episode Num: 175 Reward: 817.4147293198703\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 728.837022\n",
      "---------------------------------------\n",
      "Total Timesteps: 176000 Episode Num: 176 Reward: 790.5243239958573\n",
      "Total Timesteps: 177000 Episode Num: 177 Reward: 560.5499735292223\n",
      "Total Timesteps: 178000 Episode Num: 178 Reward: 796.1402842486074\n",
      "Total Timesteps: 179000 Episode Num: 179 Reward: 827.0818204211655\n",
      "Total Timesteps: 180000 Episode Num: 180 Reward: 718.7201257896425\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 856.046818\n",
      "---------------------------------------\n",
      "Total Timesteps: 181000 Episode Num: 181 Reward: 860.5852383249777\n",
      "Total Timesteps: 182000 Episode Num: 182 Reward: 847.1031299919183\n",
      "Total Timesteps: 183000 Episode Num: 183 Reward: -181.59104696551893\n",
      "Total Timesteps: 184000 Episode Num: 184 Reward: 660.6055278240235\n",
      "Total Timesteps: 185000 Episode Num: 185 Reward: 381.24283878754727\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 734.066175\n",
      "---------------------------------------\n",
      "Total Timesteps: 186000 Episode Num: 186 Reward: 413.6558305482994\n",
      "Total Timesteps: 187000 Episode Num: 187 Reward: 622.1427402956764\n",
      "Total Timesteps: 188000 Episode Num: 188 Reward: 756.5845199885088\n",
      "Total Timesteps: 189000 Episode Num: 189 Reward: 830.2167491632241\n",
      "Total Timesteps: 190000 Episode Num: 190 Reward: 837.8516483191381\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 788.628672\n",
      "---------------------------------------\n",
      "Total Timesteps: 191000 Episode Num: 191 Reward: 808.7260592755234\n",
      "Total Timesteps: 192000 Episode Num: 192 Reward: 850.2078115752995\n",
      "Total Timesteps: 193000 Episode Num: 193 Reward: 811.069774264083\n",
      "Total Timesteps: 194000 Episode Num: 194 Reward: 709.8678636501394\n",
      "Total Timesteps: 195000 Episode Num: 195 Reward: 811.139079998438\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 777.349017\n",
      "---------------------------------------\n",
      "Total Timesteps: 196000 Episode Num: 196 Reward: 755.9839068382815\n",
      "Total Timesteps: 197000 Episode Num: 197 Reward: 797.2150105771663\n",
      "Total Timesteps: 198000 Episode Num: 198 Reward: 709.1902874611013\n",
      "Total Timesteps: 199000 Episode Num: 199 Reward: 827.5131202693722\n",
      "Total Timesteps: 200000 Episode Num: 200 Reward: 790.7332141890422\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 843.290310\n",
      "---------------------------------------\n",
      "Total Timesteps: 201000 Episode Num: 201 Reward: 773.5708972235466\n",
      "Total Timesteps: 202000 Episode Num: 202 Reward: 790.0467763480198\n",
      "Total Timesteps: 203000 Episode Num: 203 Reward: 838.7787559737568\n",
      "Total Timesteps: 204000 Episode Num: 204 Reward: 785.5649331581197\n",
      "Total Timesteps: 205000 Episode Num: 205 Reward: 632.4172880492878\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 780.981446\n",
      "---------------------------------------\n",
      "Total Timesteps: 206000 Episode Num: 206 Reward: 755.1604088346772\n",
      "Total Timesteps: 207000 Episode Num: 207 Reward: 773.8842565406175\n",
      "Total Timesteps: 208000 Episode Num: 208 Reward: 809.0932099986184\n",
      "Total Timesteps: 209000 Episode Num: 209 Reward: 708.5726951849313\n",
      "Total Timesteps: 210000 Episode Num: 210 Reward: 680.7708204750229\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 865.186347\n",
      "---------------------------------------\n",
      "Total Timesteps: 211000 Episode Num: 211 Reward: 813.6064715837836\n",
      "Total Timesteps: 212000 Episode Num: 212 Reward: 748.2273552185767\n",
      "Total Timesteps: 213000 Episode Num: 213 Reward: 780.3062608580227\n",
      "Total Timesteps: 214000 Episode Num: 214 Reward: 733.0724820175823\n",
      "Total Timesteps: 215000 Episode Num: 215 Reward: 662.5155888555837\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 839.154913\n",
      "---------------------------------------\n",
      "Total Timesteps: 216000 Episode Num: 216 Reward: 838.6871292916978\n",
      "Total Timesteps: 217000 Episode Num: 217 Reward: 711.5894142742181\n",
      "Total Timesteps: 218000 Episode Num: 218 Reward: 737.729048657493\n",
      "Total Timesteps: 219000 Episode Num: 219 Reward: 777.3065995555537\n",
      "Total Timesteps: 220000 Episode Num: 220 Reward: 712.7636349136881\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 777.472152\n",
      "---------------------------------------\n",
      "Total Timesteps: 221000 Episode Num: 221 Reward: 776.5577795340758\n",
      "Total Timesteps: 222000 Episode Num: 222 Reward: 741.1902405559567\n",
      "Total Timesteps: 223000 Episode Num: 223 Reward: 748.0839474349974\n",
      "Total Timesteps: 224000 Episode Num: 224 Reward: 370.95845778697077\n",
      "Total Timesteps: 225000 Episode Num: 225 Reward: 779.4591154490402\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 793.606295\n",
      "---------------------------------------\n",
      "Total Timesteps: 226000 Episode Num: 226 Reward: 750.2542070176542\n",
      "Total Timesteps: 227000 Episode Num: 227 Reward: 659.3496373448493\n",
      "Total Timesteps: 228000 Episode Num: 228 Reward: 742.2877665047633\n",
      "Total Timesteps: 229000 Episode Num: 229 Reward: 758.4941411873277\n",
      "Total Timesteps: 230000 Episode Num: 230 Reward: 791.3350051066743\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 719.038318\n",
      "---------------------------------------\n",
      "Total Timesteps: 231000 Episode Num: 231 Reward: 724.9331008154877\n",
      "Total Timesteps: 232000 Episode Num: 232 Reward: 739.2780545374267\n",
      "Total Timesteps: 233000 Episode Num: 233 Reward: 695.7955285919245\n",
      "Total Timesteps: 234000 Episode Num: 234 Reward: 787.8880445312276\n",
      "Total Timesteps: 235000 Episode Num: 235 Reward: 765.0318964110908\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 867.522659\n",
      "---------------------------------------\n",
      "Total Timesteps: 236000 Episode Num: 236 Reward: 773.3656458565581\n",
      "Total Timesteps: 237000 Episode Num: 237 Reward: 844.4744682074642\n",
      "Total Timesteps: 238000 Episode Num: 238 Reward: 825.8340716576134\n",
      "Total Timesteps: 239000 Episode Num: 239 Reward: 748.1339358164355\n",
      "Total Timesteps: 240000 Episode Num: 240 Reward: 683.2028902730013\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 785.895464\n",
      "---------------------------------------\n",
      "Total Timesteps: 241000 Episode Num: 241 Reward: 781.9139833654655\n",
      "Total Timesteps: 242000 Episode Num: 242 Reward: 704.0235745290678\n",
      "Total Timesteps: 243000 Episode Num: 243 Reward: 836.8796540654574\n",
      "Total Timesteps: 244000 Episode Num: 244 Reward: 888.0736242955485\n",
      "Total Timesteps: 245000 Episode Num: 245 Reward: 876.3437969796727\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 820.658722\n",
      "---------------------------------------\n",
      "Total Timesteps: 246000 Episode Num: 246 Reward: 819.7743240030635\n",
      "Total Timesteps: 247000 Episode Num: 247 Reward: 810.4603870266043\n",
      "Total Timesteps: 248000 Episode Num: 248 Reward: 623.4011843460823\n",
      "Total Timesteps: 249000 Episode Num: 249 Reward: 861.7135588198295\n",
      "Total Timesteps: 250000 Episode Num: 250 Reward: 836.1736279728468\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 619.820599\n",
      "---------------------------------------\n",
      "Total Timesteps: 251000 Episode Num: 251 Reward: 643.9664253464213\n",
      "Total Timesteps: 252000 Episode Num: 252 Reward: 864.286074017924\n",
      "Total Timesteps: 253000 Episode Num: 253 Reward: 871.176805599707\n",
      "Total Timesteps: 254000 Episode Num: 254 Reward: 911.4373658066473\n",
      "Total Timesteps: 255000 Episode Num: 255 Reward: 781.4013099079846\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 879.805570\n",
      "---------------------------------------\n",
      "Total Timesteps: 256000 Episode Num: 256 Reward: 857.9341778471362\n",
      "Total Timesteps: 257000 Episode Num: 257 Reward: 807.2874054038476\n",
      "Total Timesteps: 258000 Episode Num: 258 Reward: 748.1825538263663\n",
      "Total Timesteps: 259000 Episode Num: 259 Reward: 816.2238042392077\n",
      "Total Timesteps: 260000 Episode Num: 260 Reward: 710.8524597801664\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 789.811916\n",
      "---------------------------------------\n",
      "Total Timesteps: 261000 Episode Num: 261 Reward: 778.0667595611528\n",
      "Total Timesteps: 262000 Episode Num: 262 Reward: 789.8258269227889\n",
      "Total Timesteps: 263000 Episode Num: 263 Reward: 896.5894431661632\n",
      "Total Timesteps: 264000 Episode Num: 264 Reward: 879.9291451319723\n",
      "Total Timesteps: 265000 Episode Num: 265 Reward: 873.4478586821124\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 845.540853\n",
      "---------------------------------------\n",
      "Total Timesteps: 266000 Episode Num: 266 Reward: 783.2140734312235\n",
      "Total Timesteps: 267000 Episode Num: 267 Reward: 673.4621436827744\n",
      "Total Timesteps: 268000 Episode Num: 268 Reward: 723.4165191271002\n",
      "Total Timesteps: 269000 Episode Num: 269 Reward: 789.7902223141997\n",
      "Total Timesteps: 270000 Episode Num: 270 Reward: 832.7935795055945\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 855.521551\n",
      "---------------------------------------\n",
      "Total Timesteps: 271000 Episode Num: 271 Reward: 860.8746422134459\n",
      "Total Timesteps: 272000 Episode Num: 272 Reward: 778.4413103934257\n",
      "Total Timesteps: 273000 Episode Num: 273 Reward: 787.2010271239964\n",
      "Total Timesteps: 274000 Episode Num: 274 Reward: 805.4009331273928\n",
      "Total Timesteps: 275000 Episode Num: 275 Reward: 760.7230184959793\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 764.444179\n",
      "---------------------------------------\n",
      "Total Timesteps: 276000 Episode Num: 276 Reward: 771.4910306957823\n",
      "Total Timesteps: 277000 Episode Num: 277 Reward: 728.4987675852651\n",
      "Total Timesteps: 278000 Episode Num: 278 Reward: 838.2942530324731\n",
      "Total Timesteps: 279000 Episode Num: 279 Reward: 890.4625519241043\n",
      "Total Timesteps: 280000 Episode Num: 280 Reward: 774.7083983669\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 676.677914\n",
      "---------------------------------------\n",
      "Total Timesteps: 281000 Episode Num: 281 Reward: 713.4927424653883\n",
      "Total Timesteps: 282000 Episode Num: 282 Reward: 764.7464193213243\n",
      "Total Timesteps: 283000 Episode Num: 283 Reward: 888.0178289655116\n",
      "Total Timesteps: 284000 Episode Num: 284 Reward: 835.405809047471\n",
      "Total Timesteps: 285000 Episode Num: 285 Reward: 842.2442826276877\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 825.172641\n",
      "---------------------------------------\n",
      "Total Timesteps: 286000 Episode Num: 286 Reward: 821.4932122913918\n",
      "Total Timesteps: 287000 Episode Num: 287 Reward: 868.9451650413212\n",
      "Total Timesteps: 288000 Episode Num: 288 Reward: 713.0301169985293\n",
      "Total Timesteps: 289000 Episode Num: 289 Reward: 829.938067317578\n",
      "Total Timesteps: 290000 Episode Num: 290 Reward: 829.5418785097604\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 741.982147\n",
      "---------------------------------------\n",
      "Total Timesteps: 291000 Episode Num: 291 Reward: 753.4534229145922\n",
      "Total Timesteps: 292000 Episode Num: 292 Reward: 864.477878095292\n",
      "Total Timesteps: 293000 Episode Num: 293 Reward: 912.44192456355\n",
      "Total Timesteps: 294000 Episode Num: 294 Reward: 844.9822764390242\n",
      "Total Timesteps: 295000 Episode Num: 295 Reward: 951.925700240637\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 863.151079\n",
      "---------------------------------------\n",
      "Total Timesteps: 296000 Episode Num: 296 Reward: 906.5852029600638\n",
      "Total Timesteps: 297000 Episode Num: 297 Reward: 853.0947409955477\n",
      "Total Timesteps: 298000 Episode Num: 298 Reward: 866.8094704449732\n",
      "Total Timesteps: 299000 Episode Num: 299 Reward: 801.2544276336064\n",
      "Total Timesteps: 300000 Episode Num: 300 Reward: 790.5086333168596\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 764.626525\n",
      "---------------------------------------\n",
      "Total Timesteps: 301000 Episode Num: 301 Reward: 757.99837837259\n",
      "Total Timesteps: 302000 Episode Num: 302 Reward: 778.1730529429241\n",
      "Total Timesteps: 303000 Episode Num: 303 Reward: 787.806928858579\n",
      "Total Timesteps: 304000 Episode Num: 304 Reward: 833.3943971045703\n",
      "Total Timesteps: 305000 Episode Num: 305 Reward: 822.4595293348608\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 863.331653\n",
      "---------------------------------------\n",
      "Total Timesteps: 306000 Episode Num: 306 Reward: 852.2107706330575\n",
      "Total Timesteps: 307000 Episode Num: 307 Reward: 857.5032904876973\n",
      "Total Timesteps: 308000 Episode Num: 308 Reward: 849.3253952546302\n",
      "Total Timesteps: 309000 Episode Num: 309 Reward: 941.6516959183447\n",
      "Total Timesteps: 310000 Episode Num: 310 Reward: 912.056388230961\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 797.586449\n",
      "---------------------------------------\n",
      "Total Timesteps: 311000 Episode Num: 311 Reward: 945.9545894579742\n",
      "Total Timesteps: 312000 Episode Num: 312 Reward: 906.6592979777249\n",
      "Total Timesteps: 313000 Episode Num: 313 Reward: 956.3511321564786\n",
      "Total Timesteps: 314000 Episode Num: 314 Reward: 975.2250855396813\n",
      "Total Timesteps: 315000 Episode Num: 315 Reward: 953.7881499313484\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 956.737577\n",
      "---------------------------------------\n",
      "Total Timesteps: 316000 Episode Num: 316 Reward: 940.5620653503613\n",
      "Total Timesteps: 317000 Episode Num: 317 Reward: 940.5427225663611\n",
      "Total Timesteps: 318000 Episode Num: 318 Reward: 1039.1956806995872\n",
      "Total Timesteps: 319000 Episode Num: 319 Reward: 1050.6038652191096\n",
      "Total Timesteps: 320000 Episode Num: 320 Reward: 1027.3683379319443\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 983.435514\n",
      "---------------------------------------\n",
      "Total Timesteps: 321000 Episode Num: 321 Reward: 1009.3420706528003\n",
      "Total Timesteps: 322000 Episode Num: 322 Reward: 952.7081341527742\n",
      "Total Timesteps: 323000 Episode Num: 323 Reward: 732.9580278399727\n",
      "Total Timesteps: 324000 Episode Num: 324 Reward: 827.4778895168541\n",
      "Total Timesteps: 325000 Episode Num: 325 Reward: 871.2581825660773\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1033.547184\n",
      "---------------------------------------\n",
      "Total Timesteps: 326000 Episode Num: 326 Reward: 1034.2993339435322\n",
      "Total Timesteps: 327000 Episode Num: 327 Reward: 1000.0331151896329\n",
      "Total Timesteps: 328000 Episode Num: 328 Reward: 937.2244664526078\n",
      "Total Timesteps: 329000 Episode Num: 329 Reward: 939.435278129194\n",
      "Total Timesteps: 330000 Episode Num: 330 Reward: 1011.8031001373862\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1014.048090\n",
      "---------------------------------------\n",
      "Total Timesteps: 331000 Episode Num: 331 Reward: 1012.7773842135173\n",
      "Total Timesteps: 332000 Episode Num: 332 Reward: 945.7703523240797\n",
      "Total Timesteps: 333000 Episode Num: 333 Reward: 941.789590492349\n",
      "Total Timesteps: 334000 Episode Num: 334 Reward: 986.9971093512219\n",
      "Total Timesteps: 335000 Episode Num: 335 Reward: 980.2706307427273\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 950.539031\n",
      "---------------------------------------\n",
      "Total Timesteps: 336000 Episode Num: 336 Reward: 939.8261599360922\n",
      "Total Timesteps: 337000 Episode Num: 337 Reward: 783.9309744304755\n",
      "Total Timesteps: 338000 Episode Num: 338 Reward: 907.1305565106281\n",
      "Total Timesteps: 339000 Episode Num: 339 Reward: 1026.7157701169997\n",
      "Total Timesteps: 340000 Episode Num: 340 Reward: 1021.3657450619871\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1065.695679\n",
      "---------------------------------------\n",
      "Total Timesteps: 341000 Episode Num: 341 Reward: 1052.0924161943938\n",
      "Total Timesteps: 342000 Episode Num: 342 Reward: 1089.54642802105\n",
      "Total Timesteps: 343000 Episode Num: 343 Reward: 1072.1780163582227\n",
      "Total Timesteps: 344000 Episode Num: 344 Reward: 1061.3424480048136\n",
      "Total Timesteps: 345000 Episode Num: 345 Reward: 1130.0441254962223\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1132.424575\n",
      "---------------------------------------\n",
      "Total Timesteps: 346000 Episode Num: 346 Reward: 1131.5549530518165\n",
      "Total Timesteps: 347000 Episode Num: 347 Reward: 912.697079469381\n",
      "Total Timesteps: 348000 Episode Num: 348 Reward: 1062.392828964722\n",
      "Total Timesteps: 349000 Episode Num: 349 Reward: 1134.7669405506297\n",
      "Total Timesteps: 350000 Episode Num: 350 Reward: 1217.2765673074264\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1158.551732\n",
      "---------------------------------------\n",
      "Total Timesteps: 351000 Episode Num: 351 Reward: 1141.716137975379\n",
      "Total Timesteps: 352000 Episode Num: 352 Reward: 1194.5857230986967\n",
      "Total Timesteps: 353000 Episode Num: 353 Reward: 1218.5908436241887\n",
      "Total Timesteps: 354000 Episode Num: 354 Reward: 1120.6471360703515\n",
      "Total Timesteps: 355000 Episode Num: 355 Reward: 1196.7123423893972\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1310.748882\n",
      "---------------------------------------\n",
      "Total Timesteps: 356000 Episode Num: 356 Reward: 1265.6500618354935\n",
      "Total Timesteps: 357000 Episode Num: 357 Reward: 1187.093383166336\n",
      "Total Timesteps: 358000 Episode Num: 358 Reward: 1164.27971808995\n",
      "Total Timesteps: 359000 Episode Num: 359 Reward: 1245.285065359745\n",
      "Total Timesteps: 360000 Episode Num: 360 Reward: 1245.5299695514084\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1271.521507\n",
      "---------------------------------------\n",
      "Total Timesteps: 361000 Episode Num: 361 Reward: 1286.177011319635\n",
      "Total Timesteps: 362000 Episode Num: 362 Reward: 1063.0763572630308\n",
      "Total Timesteps: 363000 Episode Num: 363 Reward: 1248.282003294296\n",
      "Total Timesteps: 364000 Episode Num: 364 Reward: 1298.9427956630855\n",
      "Total Timesteps: 365000 Episode Num: 365 Reward: 1323.3612930766128\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1336.471650\n",
      "---------------------------------------\n",
      "Total Timesteps: 366000 Episode Num: 366 Reward: 1295.211834196426\n",
      "Total Timesteps: 367000 Episode Num: 367 Reward: 1333.3821999069\n",
      "Total Timesteps: 368000 Episode Num: 368 Reward: 1168.1340691748837\n",
      "Total Timesteps: 369000 Episode Num: 369 Reward: 1390.4432469479302\n",
      "Total Timesteps: 370000 Episode Num: 370 Reward: 1168.8298941520588\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1413.880449\n",
      "---------------------------------------\n",
      "Total Timesteps: 371000 Episode Num: 371 Reward: 1383.677739630497\n",
      "Total Timesteps: 372000 Episode Num: 372 Reward: 1369.5528097966353\n",
      "Total Timesteps: 373000 Episode Num: 373 Reward: 1395.7882476362586\n",
      "Total Timesteps: 374000 Episode Num: 374 Reward: 1457.8394858604688\n",
      "Total Timesteps: 375000 Episode Num: 375 Reward: 1408.7389410613769\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1511.469491\n",
      "---------------------------------------\n",
      "Total Timesteps: 376000 Episode Num: 376 Reward: 1484.666677256955\n",
      "Total Timesteps: 377000 Episode Num: 377 Reward: 1313.7605778576533\n",
      "Total Timesteps: 378000 Episode Num: 378 Reward: 1400.4525337693483\n",
      "Total Timesteps: 379000 Episode Num: 379 Reward: 1324.6126336893974\n",
      "Total Timesteps: 380000 Episode Num: 380 Reward: 1439.6030425771514\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1502.265477\n",
      "---------------------------------------\n",
      "Total Timesteps: 381000 Episode Num: 381 Reward: 1466.04240051695\n",
      "Total Timesteps: 382000 Episode Num: 382 Reward: 1519.0466205438022\n",
      "Total Timesteps: 383000 Episode Num: 383 Reward: 1452.503654611885\n",
      "Total Timesteps: 384000 Episode Num: 384 Reward: 1456.5053863780217\n",
      "Total Timesteps: 385000 Episode Num: 385 Reward: 1510.3289886124844\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1602.374637\n",
      "---------------------------------------\n",
      "Total Timesteps: 386000 Episode Num: 386 Reward: 1527.4327975275583\n",
      "Total Timesteps: 387000 Episode Num: 387 Reward: 1562.809835234471\n",
      "Total Timesteps: 388000 Episode Num: 388 Reward: 1584.4591840056937\n",
      "Total Timesteps: 389000 Episode Num: 389 Reward: 1556.6875707734036\n",
      "Total Timesteps: 390000 Episode Num: 390 Reward: 1520.0209827040503\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1631.040158\n",
      "---------------------------------------\n",
      "Total Timesteps: 391000 Episode Num: 391 Reward: 1556.8420481203254\n",
      "Total Timesteps: 392000 Episode Num: 392 Reward: 1387.1852547277854\n",
      "Total Timesteps: 393000 Episode Num: 393 Reward: 1566.8076551294184\n",
      "Total Timesteps: 394000 Episode Num: 394 Reward: 1602.741545869708\n",
      "Total Timesteps: 395000 Episode Num: 395 Reward: 1566.8823155294447\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1765.661119\n",
      "---------------------------------------\n",
      "Total Timesteps: 396000 Episode Num: 396 Reward: 1670.0955560608354\n",
      "Total Timesteps: 397000 Episode Num: 397 Reward: 1452.021567984953\n",
      "Total Timesteps: 398000 Episode Num: 398 Reward: 1448.006121555046\n",
      "Total Timesteps: 399000 Episode Num: 399 Reward: 1320.772546227679\n",
      "Total Timesteps: 400000 Episode Num: 400 Reward: 1686.635108324489\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1649.999893\n",
      "---------------------------------------\n",
      "Total Timesteps: 401000 Episode Num: 401 Reward: 1581.1848001594776\n",
      "Total Timesteps: 402000 Episode Num: 402 Reward: 1656.9265551705525\n",
      "Total Timesteps: 403000 Episode Num: 403 Reward: 1437.673134223209\n",
      "Total Timesteps: 404000 Episode Num: 404 Reward: 1636.7548462576592\n",
      "Total Timesteps: 405000 Episode Num: 405 Reward: 1629.596225165734\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1696.722085\n",
      "---------------------------------------\n",
      "Total Timesteps: 406000 Episode Num: 406 Reward: 1550.444059286716\n",
      "Total Timesteps: 407000 Episode Num: 407 Reward: 1736.4252410316221\n",
      "Total Timesteps: 408000 Episode Num: 408 Reward: 1642.4023051465015\n",
      "Total Timesteps: 409000 Episode Num: 409 Reward: 1446.6083109509989\n",
      "Total Timesteps: 410000 Episode Num: 410 Reward: 1591.7274514105898\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1682.295219\n",
      "---------------------------------------\n",
      "Total Timesteps: 411000 Episode Num: 411 Reward: 1646.163895099193\n",
      "Total Timesteps: 412000 Episode Num: 412 Reward: 1578.5304218325493\n",
      "Total Timesteps: 413000 Episode Num: 413 Reward: 1635.6323138972487\n",
      "Total Timesteps: 414000 Episode Num: 414 Reward: 1558.9636991364823\n",
      "Total Timesteps: 415000 Episode Num: 415 Reward: 1620.8449050340041\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1634.775470\n",
      "---------------------------------------\n",
      "Total Timesteps: 416000 Episode Num: 416 Reward: 1595.615415282656\n",
      "Total Timesteps: 417000 Episode Num: 417 Reward: 1643.5310812114583\n",
      "Total Timesteps: 418000 Episode Num: 418 Reward: 1501.2839274591825\n",
      "Total Timesteps: 419000 Episode Num: 419 Reward: 1795.3557956149557\n",
      "Total Timesteps: 420000 Episode Num: 420 Reward: 1762.2745901990397\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1765.325922\n",
      "---------------------------------------\n",
      "Total Timesteps: 421000 Episode Num: 421 Reward: 1643.2137793104498\n",
      "Total Timesteps: 422000 Episode Num: 422 Reward: 1803.0817568706384\n",
      "Total Timesteps: 423000 Episode Num: 423 Reward: 1677.324951484744\n",
      "Total Timesteps: 424000 Episode Num: 424 Reward: 1773.6589638953942\n",
      "Total Timesteps: 425000 Episode Num: 425 Reward: 1654.1485985864147\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1890.910142\n",
      "---------------------------------------\n",
      "Total Timesteps: 426000 Episode Num: 426 Reward: 1840.9050681165372\n",
      "Total Timesteps: 427000 Episode Num: 427 Reward: 1688.121011345252\n",
      "Total Timesteps: 428000 Episode Num: 428 Reward: 1740.707043625785\n",
      "Total Timesteps: 429000 Episode Num: 429 Reward: 1705.2720536450217\n",
      "Total Timesteps: 430000 Episode Num: 430 Reward: 1707.8042632547936\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1834.949064\n",
      "---------------------------------------\n",
      "Total Timesteps: 431000 Episode Num: 431 Reward: 1797.5213175216925\n",
      "Total Timesteps: 432000 Episode Num: 432 Reward: 1823.3422972352912\n",
      "Total Timesteps: 433000 Episode Num: 433 Reward: 1821.3265969644783\n",
      "Total Timesteps: 434000 Episode Num: 434 Reward: 1842.7257180005204\n",
      "Total Timesteps: 435000 Episode Num: 435 Reward: 1874.9280141129145\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1977.544956\n",
      "---------------------------------------\n",
      "Total Timesteps: 436000 Episode Num: 436 Reward: 1840.5945708927586\n",
      "Total Timesteps: 437000 Episode Num: 437 Reward: 1908.748433579382\n",
      "Total Timesteps: 438000 Episode Num: 438 Reward: 1731.7211494465203\n",
      "Total Timesteps: 439000 Episode Num: 439 Reward: 1806.4247135495887\n",
      "Total Timesteps: 440000 Episode Num: 440 Reward: 1875.922731892881\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1836.727306\n",
      "---------------------------------------\n",
      "Total Timesteps: 441000 Episode Num: 441 Reward: 1827.8849522695064\n",
      "Total Timesteps: 442000 Episode Num: 442 Reward: 1825.9170430418312\n",
      "Total Timesteps: 443000 Episode Num: 443 Reward: 1865.8171031267377\n",
      "Total Timesteps: 444000 Episode Num: 444 Reward: 1891.6799022889604\n",
      "Total Timesteps: 445000 Episode Num: 445 Reward: 1911.7750240397843\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1914.831378\n",
      "---------------------------------------\n",
      "Total Timesteps: 446000 Episode Num: 446 Reward: 1859.4773232591356\n",
      "Total Timesteps: 447000 Episode Num: 447 Reward: 1876.5936272969764\n",
      "Total Timesteps: 448000 Episode Num: 448 Reward: 1825.858949875212\n",
      "Total Timesteps: 449000 Episode Num: 449 Reward: 1864.3504684673285\n",
      "Total Timesteps: 450000 Episode Num: 450 Reward: 1877.9674433235857\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2058.563154\n",
      "---------------------------------------\n",
      "Total Timesteps: 451000 Episode Num: 451 Reward: 1959.0058436582674\n",
      "Total Timesteps: 452000 Episode Num: 452 Reward: 1865.8179105097397\n",
      "Total Timesteps: 453000 Episode Num: 453 Reward: 1866.3431083034945\n",
      "Total Timesteps: 454000 Episode Num: 454 Reward: 1703.6199364336894\n",
      "Total Timesteps: 455000 Episode Num: 455 Reward: 1919.6935583790876\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2017.272461\n",
      "---------------------------------------\n",
      "Total Timesteps: 456000 Episode Num: 456 Reward: 1892.09041042989\n",
      "Total Timesteps: 457000 Episode Num: 457 Reward: 1923.420932329277\n",
      "Total Timesteps: 458000 Episode Num: 458 Reward: 1853.4170690301296\n",
      "Total Timesteps: 459000 Episode Num: 459 Reward: 1891.5036584844697\n",
      "Total Timesteps: 460000 Episode Num: 460 Reward: 1892.3121389502644\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2017.935957\n",
      "---------------------------------------\n",
      "Total Timesteps: 461000 Episode Num: 461 Reward: 1887.7335349739426\n",
      "Total Timesteps: 462000 Episode Num: 462 Reward: 1977.1773435148298\n",
      "Total Timesteps: 463000 Episode Num: 463 Reward: 1995.8974593638231\n",
      "Total Timesteps: 464000 Episode Num: 464 Reward: 1884.6883450218895\n",
      "Total Timesteps: 465000 Episode Num: 465 Reward: 1920.88439449113\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2059.257396\n",
      "---------------------------------------\n",
      "Total Timesteps: 466000 Episode Num: 466 Reward: 2002.5363778735589\n",
      "Total Timesteps: 467000 Episode Num: 467 Reward: 1884.9277399589855\n",
      "Total Timesteps: 468000 Episode Num: 468 Reward: 1895.6925465351303\n",
      "Total Timesteps: 469000 Episode Num: 469 Reward: 1971.5030612426342\n",
      "Total Timesteps: 470000 Episode Num: 470 Reward: 1994.6673577967222\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2086.855606\n",
      "---------------------------------------\n",
      "Total Timesteps: 471000 Episode Num: 471 Reward: 1978.0669495615987\n",
      "Total Timesteps: 472000 Episode Num: 472 Reward: 1979.5054973972633\n",
      "Total Timesteps: 473000 Episode Num: 473 Reward: 2013.9791596245693\n",
      "Total Timesteps: 474000 Episode Num: 474 Reward: 2007.1811591885087\n",
      "Total Timesteps: 475000 Episode Num: 475 Reward: 1970.843655140856\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2038.041244\n",
      "---------------------------------------\n",
      "Total Timesteps: 476000 Episode Num: 476 Reward: 1966.2423249638903\n",
      "Total Timesteps: 477000 Episode Num: 477 Reward: 1963.1358046012308\n",
      "Total Timesteps: 478000 Episode Num: 478 Reward: 1990.3215850919953\n",
      "Total Timesteps: 479000 Episode Num: 479 Reward: 1994.2995709109885\n",
      "Total Timesteps: 480000 Episode Num: 480 Reward: 1868.752008573014\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2024.250023\n",
      "---------------------------------------\n",
      "Total Timesteps: 481000 Episode Num: 481 Reward: 2016.298267364635\n",
      "Total Timesteps: 482000 Episode Num: 482 Reward: 1959.4650156036878\n",
      "Total Timesteps: 483000 Episode Num: 483 Reward: 1875.7867531397694\n",
      "Total Timesteps: 484000 Episode Num: 484 Reward: 1913.6906845619462\n",
      "Total Timesteps: 485000 Episode Num: 485 Reward: 2003.8226326748006\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2039.548292\n",
      "---------------------------------------\n",
      "Total Timesteps: 486000 Episode Num: 486 Reward: 1993.5580898911198\n",
      "Total Timesteps: 487000 Episode Num: 487 Reward: 1984.6652110985838\n",
      "Total Timesteps: 488000 Episode Num: 488 Reward: 2059.521532557817\n",
      "Total Timesteps: 489000 Episode Num: 489 Reward: 2019.9830663746468\n",
      "Total Timesteps: 490000 Episode Num: 490 Reward: 1893.274169959275\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2119.132692\n",
      "---------------------------------------\n",
      "Total Timesteps: 491000 Episode Num: 491 Reward: 1975.8492338342433\n",
      "Total Timesteps: 492000 Episode Num: 492 Reward: 2013.6325351890725\n",
      "Total Timesteps: 493000 Episode Num: 493 Reward: 2006.8024068139164\n",
      "Total Timesteps: 494000 Episode Num: 494 Reward: 2070.7182769018764\n",
      "Total Timesteps: 495000 Episode Num: 495 Reward: 2050.228763947729\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2018.728194\n",
      "---------------------------------------\n",
      "Total Timesteps: 496000 Episode Num: 496 Reward: 1999.2054996022594\n",
      "Total Timesteps: 497000 Episode Num: 497 Reward: 2051.9617258920675\n",
      "Total Timesteps: 498000 Episode Num: 498 Reward: 1991.2932539847284\n",
      "Total Timesteps: 499000 Episode Num: 499 Reward: 1994.9168195169093\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2201.972002\n",
      "---------------------------------------\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './pytorch_models/TD3_HalfCheetahBulletEnv-v0_0_actor.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 59>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# We add the last policy evaluation to our list of evaluations and we save our model\u001b[39;00m\n\u001b[0;32m     58\u001b[0m evaluations\u001b[38;5;241m.\u001b[39mappend(evaluate_policy(policy))\n\u001b[1;32m---> 59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m save_models: policy\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (file_name), directory\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./pytorch_models\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     60\u001b[0m np\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./results/\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (file_name), evaluations)\n",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36mTD3.save\u001b[1;34m(self, filename, directory)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename, directory):\n\u001b[1;32m---> 80\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[38;5;124;43m_actor.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     81\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m_critic.pth\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (directory, filename))\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py:376\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;124;03m\"\"\"save(obj, f, pickle_module=pickle, pickle_protocol=DEFAULT_PROTOCOL, _use_new_zipfile_serialization=True)\u001b[39;00m\n\u001b[0;32m    341\u001b[0m \n\u001b[0;32m    342\u001b[0m \u001b[38;5;124;03mSaves an object to a disk file.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;124;03m    >>> torch.save(x, buffer)\u001b[39;00m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    374\u001b[0m _check_dill_version(pickle_module)\n\u001b[1;32m--> 376\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m    377\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m    378\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(opened_file) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py:230\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 230\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    231\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    232\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py:211\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[1;32m--> 211\u001b[0m     \u001b[38;5;28msuper\u001b[39m(_open_file, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './pytorch_models/TD3_HalfCheetahBulletEnv-v0_0_actor.pth'"
     ]
    }
   ],
   "source": [
    "# We start the main loop over 500,000 timesteps\n",
    "while total_timesteps < max_timesteps:\n",
    "  \n",
    "    # If the episode is done\n",
    "    if done:\n",
    "\n",
    "        # If we are not at the very beginning, we start the training process of the model\n",
    "        if total_timesteps != 0:\n",
    "            print(\"Total Timesteps: {} Episode Num: {} Reward: {}\".format(total_timesteps, episode_num, episode_reward))\n",
    "            policy.train(replay_buffer, episode_timesteps, batch_size, discount, tau, policy_noise, noise_clip, policy_freq)\n",
    "\n",
    "        # We evaluate the episode and we save the policy\n",
    "        if timesteps_since_eval >= eval_freq:\n",
    "            timesteps_since_eval %= eval_freq\n",
    "            evaluations.append(evaluate_policy(policy))\n",
    "            policy.save(file_name, directory=\"TD3/pytorch_models\")\n",
    "            np.save(\"TD3/%s\" % (file_name), evaluations)\n",
    "    \n",
    "        # When the training step is done, we reset the state of the environment\n",
    "        obs = env.reset()\n",
    "    \n",
    "        # Set the Done to False\n",
    "        done = False\n",
    "    \n",
    "        # Set rewards and episode timesteps to zero\n",
    "        episode_reward = 0\n",
    "        episode_timesteps = 0\n",
    "        episode_num += 1\n",
    "  \n",
    "    # Before 10000 timesteps, we play random actions\n",
    "    if total_timesteps < start_timesteps:\n",
    "        action = env.action_space.sample()\n",
    "    else: # After 10000 timesteps, we switch to the model\n",
    "        action = policy.select_action(np.array(obs))\n",
    "        # If the explore_noise parameter is not 0, we add noise to the action and we clip it\n",
    "        if expl_noise != 0:\n",
    "            action = (action + np.random.normal(0, expl_noise, size=env.action_space.shape[0])).clip(env.action_space.low, env.action_space.high)\n",
    "  \n",
    "    # The agent performs the action in the environment, then reaches the next state and receives the reward\n",
    "    new_obs, reward, done, _ = env.step(action)\n",
    "  \n",
    "    # We check if the episode is done\n",
    "    done_bool = 0 if episode_timesteps + 1 == env._max_episode_steps else float(done)\n",
    "  \n",
    "    # We increase the total reward\n",
    "    episode_reward += reward\n",
    "  \n",
    "    # We store the new transition into the Experience Replay memory (ReplayBuffer)\n",
    "    replay_buffer.add((obs, new_obs, action, reward, done_bool))\n",
    "\n",
    "    # We update the state, the episode timestep, the total timesteps, and the timesteps since the evaluation of the policy\n",
    "    obs = new_obs\n",
    "    episode_timesteps += 1\n",
    "    total_timesteps += 1\n",
    "    timesteps_since_eval += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2196.080052\n",
      "---------------------------------------\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument: 'TD3/pytorch_models/TD3_HalfCheetahBulletEnv-v0_0_actor.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Input \u001b[1;32mIn [28]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# We add the last policy evaluation to our list of evaluations and we save our model\u001b[39;00m\n\u001b[0;32m      2\u001b[0m evaluations\u001b[38;5;241m.\u001b[39mappend(evaluate_policy(policy))\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m save_models: policy\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (file_name), directory\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTD3/pytorch_models\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m np\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./results/\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (file_name), evaluations)\n",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36mTD3.save\u001b[1;34m(self, filename, directory)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename, directory):\n\u001b[1;32m---> 80\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[38;5;124;43m_actor.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     81\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m_critic.pth\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (directory, filename))\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py:376\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;124;03m\"\"\"save(obj, f, pickle_module=pickle, pickle_protocol=DEFAULT_PROTOCOL, _use_new_zipfile_serialization=True)\u001b[39;00m\n\u001b[0;32m    341\u001b[0m \n\u001b[0;32m    342\u001b[0m \u001b[38;5;124;03mSaves an object to a disk file.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;124;03m    >>> torch.save(x, buffer)\u001b[39;00m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    374\u001b[0m _check_dill_version(pickle_module)\n\u001b[1;32m--> 376\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m    377\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m    378\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(opened_file) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py:230\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 230\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    231\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    232\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py:211\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[1;32m--> 211\u001b[0m     \u001b[38;5;28msuper\u001b[39m(_open_file, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 22] Invalid argument: 'TD3/pytorch_models/TD3_HalfCheetahBulletEnv-v0_0_actor.pth'"
     ]
    }
   ],
   "source": [
    "# We add the last policy evaluation to our list of evaluations and we save our model\n",
    "evaluations.append(evaluate_policy(policy))\n",
    "if save_models: policy.save(\"%s\" % (file_name), directory=\"TD3/pytorch_models\")\n",
    "np.save(\"./results/%s\" % (file_name), evaluations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wi6e2-_pu05e"
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oW4d1YAMqif1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Settings: TD3_HalfCheetahBulletEnv-v0_0\n",
      "---------------------------------------\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument: 'TD3/pytorch_models/TD3_HalfCheetahBulletEnv-v0_0_actor.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Input \u001b[1;32mIn [34]\u001b[0m, in \u001b[0;36m<cell line: 173>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    171\u001b[0m max_action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mhigh[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    172\u001b[0m policy \u001b[38;5;241m=\u001b[39m TD3(state_dim, action_dim, max_action)\n\u001b[1;32m--> 173\u001b[0m \u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTD3/pytorch_models\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    174\u001b[0m _ \u001b[38;5;241m=\u001b[39m evaluate_policy(policy, eval_episodes\u001b[38;5;241m=\u001b[39meval_episodes)\n",
      "Input \u001b[1;32mIn [34]\u001b[0m, in \u001b[0;36mTD3.load\u001b[1;34m(self, filename, directory)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename, directory):\n\u001b[1;32m--> 132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[38;5;124;43m_actor.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m_critic.pth\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (directory, filename)))\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py:699\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    696\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    697\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 699\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m    701\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m    702\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m    703\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m    704\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py:230\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 230\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    231\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    232\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py:211\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[1;32m--> 211\u001b[0m     \u001b[38;5;28msuper\u001b[39m(_open_file, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 22] Invalid argument: 'TD3/pytorch_models/TD3_HalfCheetahBulletEnv-v0_0_actor.pth'"
     ]
    }
   ],
   "source": [
    "class Actor(nn.Module):\n",
    "  \n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        super(Actor, self).__init__()\n",
    "        self.layer_1 = nn.Linear(state_dim, 400)\n",
    "        self.layer_2 = nn.Linear(400, 300)\n",
    "        self.layer_3 = nn.Linear(300, action_dim)\n",
    "        self.max_action = max_action\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer_1(x))\n",
    "        x = F.relu(self.layer_2(x))\n",
    "        x = self.max_action * torch.tanh(self.layer_3(x)) \n",
    "        return x\n",
    "\n",
    "class Critic(nn.Module):\n",
    "  \n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        # Defining the first Critic neural network\n",
    "        self.layer_1 = nn.Linear(state_dim + action_dim, 400)\n",
    "        self.layer_2 = nn.Linear(400, 300)\n",
    "        self.layer_3 = nn.Linear(300, 1)\n",
    "        # Defining the second Critic neural network\n",
    "        self.layer_4 = nn.Linear(state_dim + action_dim, 400)\n",
    "        self.layer_5 = nn.Linear(400, 300)\n",
    "        self.layer_6 = nn.Linear(300, 1)\n",
    "\n",
    "    def forward(self, x, u):\n",
    "        xu = torch.cat([x, u], 1)\n",
    "        # Forward-Propagation on the first Critic Neural Network\n",
    "        x1 = F.relu(self.layer_1(xu))\n",
    "        x1 = F.relu(self.layer_2(x1))\n",
    "        x1 = self.layer_3(x1)\n",
    "        # Forward-Propagation on the second Critic Neural Network\n",
    "        x2 = F.relu(self.layer_4(xu))\n",
    "        x2 = F.relu(self.layer_5(x2))\n",
    "        x2 = self.layer_6(x2)\n",
    "        return x1, x2\n",
    "\n",
    "    def Q1(self, x, u):\n",
    "        xu = torch.cat([x, u], 1)\n",
    "        x1 = F.relu(self.layer_1(xu))\n",
    "        x1 = F.relu(self.layer_2(x1))\n",
    "        x1 = self.layer_3(x1)\n",
    "        return x1\n",
    "\n",
    "# Selecting the device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Building the whole Training Process into a class\n",
    "\n",
    "class TD3(object):\n",
    "  \n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
    "        self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n",
    "        self.critic = Critic(state_dim, action_dim).to(device)\n",
    "        self.critic_target = Critic(state_dim, action_dim).to(device)\n",
    "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
    "        self.max_action = max_action\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.Tensor(state.reshape(1, -1)).to(device)\n",
    "        return self.actor(state).cpu().data.numpy().flatten()\n",
    "\n",
    "    def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
    "    \n",
    "        for it in range(iterations):\n",
    "      \n",
    "            # Step 4: We sample a batch of transitions (s, s’, a, r) from the memory\n",
    "            batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
    "            state = torch.Tensor(batch_states).to(device)\n",
    "            next_state = torch.Tensor(batch_next_states).to(device)\n",
    "            action = torch.Tensor(batch_actions).to(device)\n",
    "            reward = torch.Tensor(batch_rewards).to(device)\n",
    "            done = torch.Tensor(batch_dones).to(device)\n",
    "      \n",
    "            # Step 5: From the next state s’, the Actor target plays the next action a’\n",
    "            next_action = self.actor_target(next_state)\n",
    "      \n",
    "            # Step 6: We add Gaussian noise to this next action a’ and we clamp it in a range of values supported by the environment\n",
    "            noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)\n",
    "            noise = noise.clamp(-noise_clip, noise_clip)\n",
    "            next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n",
    "      \n",
    "            # Step 7: The two Critic targets take each the couple (s’, a’) as input and return two Q-values Qt1(s’,a’) and Qt2(s’,a’) as outputs\n",
    "            target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
    "      \n",
    "            # Step 8: We keep the minimum of these two Q-values: min(Qt1, Qt2)\n",
    "            target_Q = torch.min(target_Q1, target_Q2)\n",
    "      \n",
    "            # Step 9: We get the final target of the two Critic models, which is: Qt = r + γ * min(Qt1, Qt2), where γ is the discount factor\n",
    "            target_Q = reward + ((1 - done) * discount * target_Q).detach()\n",
    "      \n",
    "            # Step 10: The two Critic models take each the couple (s, a) as input and return two Q-values Q1(s,a) and Q2(s,a) as outputs\n",
    "            current_Q1, current_Q2 = self.critic(state, action)\n",
    "      \n",
    "            # Step 11: We compute the loss coming from the two Critic models: Critic Loss = MSE_Loss(Q1(s,a), Qt) + MSE_Loss(Q2(s,a), Qt)\n",
    "            critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
    "      \n",
    "            # Step 12: We backpropagate this Critic loss and update the parameters of the two Critic models with a SGD optimizer\n",
    "            self.critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            self.critic_optimizer.step()\n",
    "      \n",
    "            # Step 13: Once every two iterations, we update our Actor model by performing gradient ascent on the output of the first Critic model\n",
    "            if it % policy_freq == 0:\n",
    "                actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
    "                self.actor_optimizer.zero_grad()\n",
    "                actor_loss.backward()\n",
    "                self.actor_optimizer.step()\n",
    "        \n",
    "                # Step 14: Still once every two iterations, we update the weights of the Actor target by polyak averaging\n",
    "                for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "                    target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "        \n",
    "                # Step 15: Still once every two iterations, we update the weights of the Critic target by polyak averaging\n",
    "                for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "                    target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "  \n",
    "    # Making a save method to save a trained model\n",
    "    def save(self, filename, directory):\n",
    "        torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
    "        torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
    "  \n",
    "    # Making a load method to load a pre-trained model\n",
    "    def load(self, filename, directory):\n",
    "        self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
    "        self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))\n",
    "\n",
    "def evaluate_policy(policy, eval_episodes=10):\n",
    "    avg_reward = 0.\n",
    "    for _ in range(eval_episodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = policy.select_action(np.array(obs))\n",
    "            obs, reward, done, _ = env.step(action)\n",
    "            avg_reward += reward\n",
    "    \n",
    "    avg_reward /= eval_episodes\n",
    "    print (\"---------------------------------------\")\n",
    "    print (\"Average Reward over the Evaluation Step: %f\" % (avg_reward))\n",
    "    print (\"---------------------------------------\")\n",
    "    return avg_reward\n",
    "\n",
    "env_name = \"HalfCheetahBulletEnv-v0\"\n",
    "seed = 0\n",
    "\n",
    "file_name = \"%s_%s_%s\" % (\"TD3\", env_name, str(seed))\n",
    "print (\"---------------------------------------\")\n",
    "print (\"Settings: %s\" % (file_name))\n",
    "print (\"---------------------------------------\")\n",
    "\n",
    "eval_episodes = 10\n",
    "save_env_vid = True\n",
    "env = gym.make(env_name)\n",
    "max_episode_steps = env._max_episode_steps\n",
    "if save_env_vid:\n",
    "    env = wrappers.Monitor(env, monitor_dir, force = True)\n",
    "    env.reset()\n",
    "env.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "max_action = float(env.action_space.high[0])\n",
    "policy = TD3(state_dim, action_dim, max_action)\n",
    "policy.load(file_name, 'TD3/pytorch_models')\n",
    "_ = evaluate_policy(policy, eval_episodes=eval_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TD3_Half_Cheetah.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
