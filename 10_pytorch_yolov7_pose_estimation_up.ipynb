{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e64819bc-4990-460c-a2ac-2b1d56f30aec",
   "metadata": {},
   "source": [
    "[**Paper**](https://arxiv.org/ftp/arxiv/papers/2204/2204.06806.pdf): YOLO-Pose: Enhancing YOLO for Multi Person Pose Estimation Using Object Keypoint Similarity Loss\n",
    "\n",
    "[**git**](https://github.com/WongKinYiu/yolov7/tree/pose)\n",
    "\n",
    "[**yolov7 detectron**](https://github.com/duongtrung/yolov7_d2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1a31ae-d04b-4401-9cd8-02eb736d939f",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "430e47b0-76ad-4514-9259-77671b9380ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "from utils.datasets import letterbox\n",
    "from utils.general import non_max_suppression_kpt\n",
    "from utils.plots import output_to_keypoint, plot_skeleton_kpts\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f414ad3f-e57a-4971-a28a-4c0f9ec4f67f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1.12.1', '4.6.0', '3.9.13')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from platform import python_version\n",
    "torch.__version__, cv2.__version__, python_version()\n",
    "# ('1.12.1', '4.6.0', '3.9.13')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b068e8-dae6-4e7a-8c68-3a8fd22b59ee",
   "metadata": {},
   "source": [
    "## Pose prerequisites\n",
    "\n",
    "Download the pre-trained pose estimation model from YOLOv7 official release\n",
    "\n",
    "wget https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-w6-pose.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65d3b5b7-314a-4382-ae97-88c6ffcfd70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model from pre-trained version\n",
    "weigths = torch.load('D:\\gdrive\\yolov7-pose\\models\\yolov7-w6-pose.pt') # replace with your path\n",
    "model = weigths['model']\n",
    "#model = model.float().to(device) # get CUDA out of memory on RTX 3080\n",
    "model = model.float().to(device=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6c993a-1409-47c6-a03a-3e2efe3ded73",
   "metadata": {},
   "source": [
    "## Test on image files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2ff538e5-2845-41ec-a606-824645850589",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pose_estimation(model, image_path):\n",
    "    \n",
    "    image = cv2.imread(image_path)\n",
    "    image = letterbox(image, 960, stride=64, auto=True)[0]\n",
    "    image = transforms.ToTensor()(image)\n",
    "    image = torch.tensor(np.array([image.numpy()]))\n",
    "    #image = image.to(device)\n",
    "    \n",
    "    # set evaluation mode\n",
    "    _ = model.eval()\n",
    "    output, _ = model(image)\n",
    "    \n",
    "    output = non_max_suppression_kpt(output, 0.25, 0.65, nc=model.yaml['nc'], nkpt=model.yaml['nkpt'], kpt_label=True)\n",
    "    with torch.no_grad():\n",
    "        output = output_to_keypoint(output)\n",
    "    nimg = image[0].permute(1, 2, 0) * 255\n",
    "    \n",
    "    nimg = nimg.cpu().numpy().astype(np.uint8)\n",
    "    nimg = cv2.cvtColor(nimg, cv2.COLOR_RGB2BGR)\n",
    "    for idx in range(output.shape[0]):\n",
    "        plot_skeleton_kpts(nimg, output[idx, 7:].T, 3)\n",
    "        \n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(nimg)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b74249f1-e3f9-42c8-8940-62c44a0182cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nError on GTX 3080\\nRuntimeError: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 0; 10.00 GiB total capacity; 9.08 GiB already allocated; 0 bytes free; 9.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  \\nSee documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\\n\\nDuring the experiment, I got CUDA out of memory error. My card is RTX 3080 10GB, belong to a Lenovo Legion Workstation\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Error on GTX 3080\n",
    "RuntimeError: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 0; 10.00 GiB total capacity; 9.08 GiB already allocated; 0 bytes free; 9.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  \n",
    "See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
    "\n",
    "During the experiment, I got CUDA out of memory error. My card is RTX 3080 10GB, belong to a Lenovo Legion Workstation\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f6c8736e-1ca9-4297-9c26-da007d23119c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "321435944"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_model_memory_consumption(model):\n",
    "    mem_params = sum([param.nelement()*param.element_size() for param in model.parameters()])\n",
    "    mem_bufs = sum([buf.nelement()*buf.element_size() for buf in model.buffers()])\n",
    "    mem = mem_params + mem_bufs # in bytes\n",
    "    return mem\n",
    "\n",
    "check_model_memory_consumption(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "17312804-3ee6-403e-ac2b-584f655ed7e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "334408704"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.max_memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "df48bd88-81e9-4220-9eb1-1bf0e894e19f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 14% |\n",
      "GPU Usage after emptying the cache\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 11% | 14% |\n"
     ]
    }
   ],
   "source": [
    "#pip install GPUtil\n",
    "\n",
    "def free_gpu_cache():\n",
    "    from GPUtil import showUtilization as gpu_usage\n",
    "    from numba import cuda\n",
    "    print(\"Initial GPU Usage\")\n",
    "    gpu_usage()                             \n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    cuda.select_device(0)\n",
    "    cuda.close()\n",
    "    cuda.select_device(0)\n",
    "\n",
    "    print(\"GPU Usage after emptying the cache\")\n",
    "    gpu_usage()\n",
    "\n",
    "free_gpu_cache()       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7adaaf3b-e542-442f-8549-5456bedff4bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lena\\AppData\\Local\\Temp\\ipykernel_11824\\3401849301.py:26: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# install GUI backend in Jupter Notebook.\n",
    "# conda install pyqt\n",
    "plot_pose_estimation(model, 'yolov7/images/team1.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8692c76a-cf2c-44ae-b2a9-20ef92a27efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lena\\AppData\\Local\\Temp\\ipykernel_11824\\3401849301.py:26: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "plot_pose_estimation(model, 'yolov7/images/team2.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b9edc954-353d-46b6-a638-1fafe0b53041",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lena\\AppData\\Local\\Temp\\ipykernel_11824\\3401849301.py:26: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "plot_pose_estimation(model, 'yolov7/images/team3.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1afc641d-a590-4350-8f98-5761825e2676",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lena\\AppData\\Local\\Temp\\ipykernel_11824\\3401849301.py:26: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "plot_pose_estimation(model, 'yolov7/images/aerobic-stepper.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd17dedf-7c47-4c29-adc4-1b6a6a4878f3",
   "metadata": {},
   "source": [
    "## Test on video files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe96ea9d-5566-4fc3-afcb-f6a585be7490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install -c conda-forge opencv\n",
    "# pip install opencv-contrib-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e20a616b-6b89-4f25-9b8f-c5ce5076f179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error while trying to read video. Please check path again\n"
     ]
    }
   ],
   "source": [
    "video_path = 'yolov7\\images\\football1.wmv'\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "if (cap.isOpened() == False):\n",
    "    print('Error while trying to read video. Please check path again')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e6680f5a-af97-4cb2-8130-9b1754c2aead",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the frame width and height.\n",
    "frame_width = int(cap.get(3))\n",
    "frame_height = int(cap.get(4))\n",
    "frame_width, frame_height "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "38ae6017-5429-44b0-a959-4cee7b2d65a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass the first frame through `letterbox` function to get the resized image,\n",
    "# to be used for `VideoWriter` dimensions. Resize by larger side.\n",
    "vid_write_image = letterbox(cap.read()[1], (frame_width), stride=64, auto=True)[0]\n",
    "resize_height, resize_width = vid_write_image.shape[:2]\n",
    "\n",
    "save_name = f\"{video_path.split('/')[-1].split('.')[0]}\"\n",
    "# Define codec and create VideoWriter object .\n",
    "out = cv2.VideoWriter(f\"{save_name}_keypoint.mp4\",\n",
    "                    cv2.VideoWriter_fourcc(*'mp4v'), 30,\n",
    "                    (resize_width, resize_height))\n",
    "\n",
    "frame_count = 0 # To count total frames.\n",
    "total_fps = 0 # To get the final frames per second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "301f3985-fd4f-4dbc-8476-5c13fd077e2e",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 15 but got size 16 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8168\\3153054706.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m             \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[1;31m# Get the end time.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mend_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\My Drive\\github-nghia\\Pytorch-tutorials\\models\\yolo.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, augment, profile)\u001b[0m\n\u001b[0;32m    597\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m  \u001b[1;31m# augmented inference, train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 599\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_once\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprofile\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# single-scale inference, train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    600\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    601\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward_once\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprofile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\My Drive\\github-nghia\\Pytorch-tutorials\\models\\yolo.py\u001b[0m in \u001b[0;36mforward_once\u001b[1;34m(self, x, profile)\u001b[0m\n\u001b[0;32m    623\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'%10.1f%10.0f%10.1fms %-40s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    624\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 625\u001b[1;33m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# run\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    626\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m             \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# save output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\My Drive\\github-nghia\\Pytorch-tutorials\\models\\common.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 15 but got size 16 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "while(cap.isOpened):\n",
    "    # Capture each frame of the video.\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        orig_image = frame\n",
    "        image = cv2.cvtColor(orig_image, cv2.COLOR_BGR2RGB)\n",
    "        image = letterbox(image, (frame_width), stride=64, auto=True)[0] \n",
    "        image = transforms.ToTensor()(image)\n",
    "        image = torch.tensor(np.array([image.numpy()]))\n",
    "        #image = image.to(device)\n",
    "        image = image.float()\n",
    "\n",
    "        # set evaluation mode\n",
    "        _ = model.eval()\n",
    "        \n",
    "        # Get the start time\n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            output, _ = model(image)\n",
    "        # Get the end time.\n",
    "        end_time = time.time()\n",
    "        # Get the fps.\n",
    "        fps = 1 / (end_time - start_time)\n",
    "        # Add fps to total fps.\n",
    "        total_fps += fps\n",
    "        # Increment frame count.\n",
    "        frame_count += 1\n",
    "\n",
    "        output = non_max_suppression_kpt(output, 0.25, 0.65, nc=model.yaml['nc'], nkpt=model.yaml['nkpt'], kpt_label=True)\n",
    "        output = output_to_keypoint(output)\n",
    "        nimg = image[0].permute(1, 2, 0) * 255\n",
    "        nimg = nimg.cpu().numpy().astype(np.uint8)\n",
    "        nimg = cv2.cvtColor(nimg, cv2.COLOR_RGB2BGR)\n",
    "        for idx in range(output.shape[0]):\n",
    "            plot_skeleton_kpts(nimg, output[idx, 7:].T, 3)\n",
    "\n",
    "            # Comment/Uncomment the following lines to show bounding boxes around persons.\n",
    "            xmin, ymin = (output[idx, 2]-output[idx, 4]/2), (output[idx, 3]-output[idx, 5]/2)\n",
    "            xmax, ymax = (output[idx, 2]+output[idx, 4]/2), (output[idx, 3]+output[idx, 5]/2)\n",
    "            cv2.rectangle(\n",
    "                      nimg,\n",
    "                      (int(xmin), int(ymin)),\n",
    "                      (int(xmax), int(ymax)),\n",
    "                      color=(255, 0, 0),\n",
    "                      thickness=1,\n",
    "                      lineType=cv2.LINE_AA\n",
    "            )\n",
    "\n",
    "        # Write the FPS on the current frame.\n",
    "        cv2.putText(nimg, f\"{fps:.3f} FPS\", (15, 30), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                      1, (0, 255, 0), 2)\n",
    "        # Convert from BGR to RGB color format.\n",
    "        cv2.imshow('image', nimg)\n",
    "        out.write(nimg)\n",
    "        # Press `q` to exit.\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b49aab1-4804-4747-80d8-d459e0b822c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Release VideoCapture().\n",
    "cap.release()\n",
    "# Close all frames and video windows.\n",
    "cv2.destroyAllWindows()\n",
    "# Calculate and print the average FPS.\n",
    "avg_fps = total_fps / frame_count\n",
    "print(f\"Average FPS: {avg_fps:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
