{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b7ee10f-b0e6-4c13-9dd8-165b4f5ed71e",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/duongtrung/Pytorch-tutorials/blob/main/06_pytorch_transfer_learning_up.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf4bd61-e32c-4663-bd90-f0f97d967b9c",
   "metadata": {},
   "source": [
    "# 06. PyTorch Transfer Learning\n",
    "\n",
    "> **Note:** This notebook uses `torchvision`'s new [multi-weight support API (available in `torchvision` v0.13+)](https://pytorch.org/blog/introducing-torchvision-new-multi-weight-support-api/).\n",
    "\n",
    "We've built a few models by hand so far, but you might be thinking, **is there a well-performing model that already exists for our problem?**\n",
    "\n",
    "And in the world of deep learning, the answer is often *yes*.\n",
    "\n",
    "We'll see how by using a powerful technique called [**transfer learning**](https://developers.google.com/machine-learning/glossary#transfer-learning)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6126b322-8dd7-4ad7-9177-702981ba9197",
   "metadata": {},
   "source": [
    "## What is transfer learning?\n",
    "\n",
    "**Transfer learning** allows us to take the patterns (also called weights) another model has learned from another problem and use them for our own problem.\n",
    "\n",
    "For example, we can take the patterns a computer vision model has learned from datasets such as [ImageNet](https://www.image-net.org/) (millions of images of different objects) and use them to power our FoodVision Mini model.\n",
    "\n",
    "Or we could take the patterns from a [language model](https://developers.google.com/machine-learning/glossary#masked-language-model) (a model that's been through large amounts of text to learn a representation of language) and use them as the basis of a model to classify different text samples.\n",
    "\n",
    "The premise remains: find a well-performing existing model and apply it to your own problem.\n",
    "\n",
    "<img src=\"resources/06-transfer-learning-example-overview.png\" alt=\"transfer learning overview on different problems\" width=600/>\n",
    "\n",
    "*Example of transfer learning being applied to computer vision and natural language processing (NLP). In the case of computer vision, a computer vision model might learn patterns on millions of images in ImageNet and then use those patterns to infer on another problem. And for NLP, a language model may learn the structure of language by reading all of Wikipedia (and perhaps more) and then apply that knowledge to a different problem.*\n",
    "\n",
    "[hands on guide to transfer learning](https://towardsdatascience.com/a-comprehensive-hands-on-guide-to-transfer-learning-with-real-world-applications-in-deep-learning-212bf3b2f27a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5485eb44-9876-45ff-8f1b-085264e435be",
   "metadata": {},
   "source": [
    "## Why use transfer learning?\n",
    "\n",
    "There are two main benefits to using transfer learning:\n",
    "\n",
    "1. Can leverage an existing model (usually a neural network architecture) proven to work on problems similar to our own.\n",
    "2. Can leverage a working model which has **already learned** patterns on similar data to our own. This often results in achieving **great results with less custom data**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ab6ca8-ee23-4858-bff6-f26fe6c3ddb2",
   "metadata": {},
   "source": [
    "## Where to find pretrained models\n",
    "\n",
    "The world of deep learning is an amazing place.\n",
    "\n",
    "So amazing that many people around the world share their work.\n",
    "\n",
    "Often, code and pretrained models for the latest state-of-the-art research is released within a few days of publishing.\n",
    "\n",
    "And there are several places you can find pretrained models to use for your own problems.\n",
    "\n",
    "| **Location** | **What's there?** | **Link(s)** | \n",
    "| ----- | ----- | ----- |\n",
    "| **PyTorch domain libraries** | Each of the PyTorch domain libraries (`torchvision`, `torchtext`) come with pretrained models of some form. The models there work right within PyTorch. | [`torchvision.models`](https://pytorch.org/vision/stable/models.html), [`torchtext.models`](https://pytorch.org/text/main/models.html), [`torchaudio.models`](https://pytorch.org/audio/stable/models.html), [`torchrec.models`](https://pytorch.org/torchrec/torchrec.models.html) |\n",
    "| **HuggingFace Hub** | A series of pretrained models on many different domains (vision, text, audio and more) from organizations around the world. There's plenty of different datasets too. | https://huggingface.co/models, https://huggingface.co/datasets | \n",
    "| **`timm` (PyTorch Image Models) library** | Almost all of the latest and greatest computer vision models in PyTorch code as well as plenty of other helpful computer vision features. | https://github.com/rwightman/pytorch-image-models|\n",
    "| **Paperswithcode** | A collection of the latest state-of-the-art machine learning papers with code implementations attached. You can also find benchmarks here of model performance on different tasks. | https://paperswithcode.com/ | \n",
    "\n",
    "*With access to such high-quality resources as above, it should be common practice at the start of every deep learning problem you take on to ask, \"Does a pretrained model exist for my problem?\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48af4e7c-d616-4400-a5cc-1054666b3914",
   "metadata": {},
   "source": [
    "## What we're going to cover\n",
    "\n",
    "We're going to take a pretrained model from `torchvision.models` and customise it to work on (and hopefully improve) our FoodVision Mini problem.\n",
    "\n",
    "| **Topic** | **Contents** |\n",
    "| ----- | ----- |\n",
    "| **0. Getting setup** | We've written a fair bit of useful code over the past few sections, let's download it and make sure we can use it again. |\n",
    "| **1. Get data** | Let's get the pizza, steak and sushi image classification dataset we've been using to try and improve our model's results. |\n",
    "| **2. Create Datasets and DataLoaders** | We'll use the `data_setup.py` script we wrote in chapter 05. PyTorch Going Modular to setup our DataLoaders. |\n",
    "| **3. Get and customise a pretrained model** | Here we'll download a pretrained model from `torchvision.models` and customise it to our own problem. | \n",
    "| **4. Train model** | Let's see how the new pretrained model goes on our pizza, steak, sushi dataset. We'll use the training functions we created in the previous chapter. |\n",
    "| **5. Evaluate the model by plotting loss curves** | How did our first transfer learning model go? Did it overfit or underfit?  |\n",
    "| **6. Make predictions on images from the test set** | It's one thing to check out a model's evaluation metrics but it's another thing to view its predictions on test samples, let's *visualize, visualize, visualize*! |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58399ad-9ff2-4520-a032-5cc3c3e01571",
   "metadata": {},
   "source": [
    "## 0. Getting setup\n",
    "\n",
    "Let's get started by importing/downloading the required modules for this section.\n",
    "\n",
    "We'll also get the [`torchinfo`](https://github.com/TylerYep/torchinfo) package if it's not available. \n",
    "\n",
    "`torchinfo` will help later on to give us a visual representation of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d652862-1d2f-4155-8f64-69bb3c79bfdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 1.12.1\n",
      "torchvision version: 0.13.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "assert int(torch.__version__.split(\".\")[1]) >= 12, \"torch version should be 1.12+\"\n",
    "assert int(torchvision.__version__.split(\".\")[1]) >= 13, \"torchvision version should be 0.13+\"\n",
    "print(f\"torch version: {torch.__version__}\")\n",
    "print(f\"torchvision version: {torchvision.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc9f6962-3843-4d19-b69b-a36e0e0c64f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue with regular imports\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "\n",
    "# Try to get torchinfo, install it if it doesn't work\n",
    "try:\n",
    "    from torchinfo import summary\n",
    "except:\n",
    "    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n",
    "    !pip install -q torchinfo\n",
    "    from torchinfo import summary\n",
    "\n",
    "# Try to import the going_modular directory\n",
    "from going_modular.going_modular import data_setup, engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aedde68a-fd18-4fa3-a72a-dbaca97b7b22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup device agnostic code\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbc239c-f4d3-4586-b598-d5d939da78cc",
   "metadata": {},
   "source": [
    "## 1. Get data\n",
    "\n",
    "Before we can start to use **transfer learning**, we'll need a dataset.\n",
    "\n",
    "To see how transfer learning compares to our previous attempts at model building, we'll use the same dataset as in [04_pytorch_custom_datasets_up.ipynb](04_pytorch_custom_datasets_up.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc25fddd-18ce-496d-ae5d-f5e9a7bab14f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\gdrive\\DataCenter\\pizza-steak-sushi directory exists.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import requests\n",
    "\n",
    "# Setup path to data folder\n",
    "data_path = Path(\"D:/gdrive/DataCenter/\")\n",
    "image_path = data_path / \"pizza-steak-sushi\"\n",
    "\n",
    "# If the image folder doesn't exist, download it and prepare it... \n",
    "if image_path.is_dir():\n",
    "    print(f\"{image_path} directory exists.\")\n",
    "else:\n",
    "    print(f\"Did not find {image_path} directory, creating one...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e60ce83d-da08-4af7-abc3-bc77116d1d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Dirs\n",
    "train_dir = image_path / \"train\"\n",
    "test_dir = image_path / \"test\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b965dd-dfc7-4da5-a5db-63898e0b089e",
   "metadata": {},
   "source": [
    "## 2. Create Datasets and DataLoaders\n",
    "\n",
    "We are going to use files in the folder going_modular.\n",
    "\n",
    "But since we'll be using a pretrained model from [`torchvision.models`](https://pytorch.org/vision/stable/models.html), there's a specific transform we need to prepare our images first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8590e2e-9a9a-418b-8469-7ccfa406f163",
   "metadata": {},
   "source": [
    "### 2.1 Creating a transform for `torchvision.models` (manual creation)\n",
    "\n",
    "> **Note:** As of `torchvision` v0.13+, there's an update to how data transforms can be created using `torchvision.models`. I've called the previous method \"manual creation\" and the new method \"auto creation\". This notebook showcases both.\n",
    "\n",
    "When using a pretrained model, it's important that **your custom data going into the model is prepared in the same way as the original training data that went into the model**.\n",
    "\n",
    "Prior to `torchvision` v0.13+, to create a transform for a pretrained model in `torchvision.models`, the documentation stated:\n",
    "\n",
    "> All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. \n",
    ">\n",
    "> The images have to be loaded in to a range of `[0, 1]` and then normalized using `mean = [0.485, 0.456, 0.406]` and `std = [0.229, 0.224, 0.225]`. \n",
    ">\n",
    "> You can use the following transform to normalize:\n",
    ">\n",
    "> ```\n",
    "> normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    ">                                  std=[0.229, 0.224, 0.225])\n",
    "> ```\n",
    "\n",
    "The good news is, we can achieve the above transformations with a combination of: \n",
    "\n",
    "| **Transform number** | **Transform required** | **Code to perform transform** | \n",
    "| ----- | ----- | ----- |\n",
    "| 1 | Mini-batches of size `[batch_size, 3, height, width]` where height and width are at least 224x224^. | `torchvision.transforms.Resize()` to resize images into `[3, 224, 224]`^ and `torch.utils.data.DataLoader()` to create batches of images. |\n",
    "| 2 | Values between 0 & 1. | `torchvision.transforms.ToTensor()` |\n",
    "| 3 | A mean of `[0.485, 0.456, 0.406]` (values across each colour channel). | `torchvision.transforms.Normalize(mean=...)` to adjust the mean of our images.  |\n",
    "| 4 | A standard deviation of `[0.229, 0.224, 0.225]` (values across each colour channel). | `torchvision.transforms.Normalize(std=...)` to adjust the standard deviation of our images.  | \n",
    "\n",
    "> **Note:** ^some pretrained models from `torchvision.models` in different sizes to `[3, 224, 224]`, for example, some might take them in `[3, 240, 240]`. For specific input image sizes, see the documentation.\n",
    "\n",
    "> **Question:** *Where did the mean and standard deviation values come from? Why do we need to do this?*\n",
    ">\n",
    "> These were calculated from the data. Specifically, the ImageNet dataset by taking the means and standard deviations across a subset of images.\n",
    ">\n",
    "> We also don't *need* to do this. Neural networks are usually quite capable of figuring out appropriate data distributions (they'll calculate where the mean and standard deviations need to be on their own) but setting them at the start can help our networks achieve better performance quicker.\n",
    "\n",
    "Let's compose a series of `torchvision.transforms` to perform the above steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f1fba8b-f1a7-4565-b83f-8822097d1a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a transforms pipeline manually (required for torchvision < 0.13)\n",
    "manual_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)), # 1. Reshape all images to 224x224 (though some models may require different sizes)\n",
    "    transforms.ToTensor(), # 2. Turn image values to between 0 & 1 \n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], # 3. A mean of [0.485, 0.456, 0.406] (across each colour channel)\n",
    "                         std=[0.229, 0.224, 0.225]) # 4. A standard deviation of [0.229, 0.224, 0.225] (across each colour channel),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a848445-0bd4-48e1-9fbc-d8d7c4a71242",
   "metadata": {},
   "source": [
    "Now we've got a **manually created series of transforms** ready to prepare our images, let's create training and testing DataLoaders.\n",
    "\n",
    "We can create these using the `create_dataloaders` function from the [`data_setup.py`](going_modular/going_modular/data_setup.py).\n",
    "\n",
    "We'll set `batch_size=32` so our model see's mini-batches of 32 samples at a time.\n",
    "\n",
    "And we can transform our images using the transform pipeline we created above by setting `transform=simple_transform`.\n",
    "\n",
    "> **Note:** I've included this manual creation of transforms in this notebook because you may come across resources that use this style. It's also important to note that because these transforms are manually created, they're also infinitely customizable. So if you wanted to included data augmentation techniques in your transforms pipeline, you could."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f76578b-d790-4068-801a-ac635ef549f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x24b5a34f910>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x24b5ca74700>,\n",
       " ['pizza', 'steak', 'sushi'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create training and testing DataLoaders as well as get a list of class names\n",
    "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n",
    "                                                                               test_dir=test_dir,\n",
    "                                                                               transform=manual_transforms, # resize, convert images to between 0 & 1 and normalize them\n",
    "                                                                               batch_size=32) # set mini-batch size to 32\n",
    "\n",
    "train_dataloader, test_dataloader, class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5669cf6e-4903-49fb-8f3d-eb72a895348b",
   "metadata": {},
   "source": [
    "### 2.2 Creating a transform for `torchvision.models` (auto creation)\n",
    "\n",
    "As previously stated, when using a pretrained model, it's important that **your custom data going into the model is prepared in the same way as the original training data that went into the model**.\n",
    "\n",
    "Above we saw how to manually create a transform for a pretrained model.\n",
    "\n",
    "But as of `torchvision` v0.13+, an automatic transform creation feature has been added.\n",
    "\n",
    "When you setup a model from `torchvision.models` and select the pretrained model weights you'd like to use, for example, say we'd like to use:\n",
    "    \n",
    "```python\n",
    "weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n",
    "```\n",
    "\n",
    "Where,\n",
    "* `EfficientNet_B0_Weights` is the model architecture weights we'd like to use (there are many differnt model architecture options in `torchvision.models`).\n",
    "* `DEFAULT` means the *best available* weights (the best performance in ImageNet).\n",
    "    * **Note:** Depending on the model architecture you choose, you may also see other options such as `IMAGENET_V1` and `IMAGENET_V2` where generally the higher version number the better. Though if you want the best available, `DEFAULT` is the easiest option. See the [`torchvision.models` documentation](https://pytorch.org/vision/main/models.html) for more.\n",
    "    \n",
    "Let's try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1efec90-bce5-4208-a403-f941a434a31e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EfficientNet_B0_Weights.IMAGENET1K_V1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a set of pretrained model weights\n",
    "weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT # .DEFAULT = best available weights from pretraining on ImageNet\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f36cde27-074f-4774-b5c3-514b15d2c3e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassification(\n",
       "    crop_size=[224]\n",
       "    resize_size=[256]\n",
       "    mean=[0.485, 0.456, 0.406]\n",
       "    std=[0.229, 0.224, 0.225]\n",
       "    interpolation=InterpolationMode.BICUBIC\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the transforms used to create our pretrained weights\n",
    "auto_transforms = weights.transforms()\n",
    "auto_transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3b25ed-73b0-4e60-9b45-e5b54f22335c",
   "metadata": {},
   "source": [
    "Notice how `auto_transforms` is very similar to `manual_transforms`, the only difference is that `auto_transforms` came with the model architecture we chose, where as we had to create `manual_transforms` by hand.\n",
    "\n",
    "The benefit of automatically creating a transform through `weights.transforms()` is that you ensure you're using the same data transformation as the pretrained model used when it was trained.\n",
    "\n",
    "However, the tradeoff of using automatically created transforms is a lack of customization.\n",
    "\n",
    "We can use `auto_transforms` to create DataLoaders with `create_dataloaders()` just as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e9a2d75-a93a-40bf-8509-2d221b0d6562",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x24b5b3a5670>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x24b5ca74b20>,\n",
       " ['pizza', 'steak', 'sushi'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create training and testing DataLoaders as well as get a list of class names\n",
    "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n",
    "                                                                               test_dir=test_dir,\n",
    "                                                                               transform=auto_transforms, # perform same data transforms on our own data as the pretrained model\n",
    "                                                                               batch_size=32) # set mini-batch size to 32\n",
    "\n",
    "train_dataloader, test_dataloader, class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1060013f-2e4e-4a66-a3c5-557242ca6c22",
   "metadata": {},
   "source": [
    "## 3. Getting a pretrained model\n",
    "\n",
    "Alright, here comes the fun part!\n",
    "\n",
    "Over the past few notebooks we've been building PyTorch neural networks from scratch.\n",
    "\n",
    "And while that's a good skill to have, our models haven't been performing as well as we'd like. \n",
    "\n",
    "That's where **transfer learning** comes in.\n",
    "\n",
    "The whole idea of transfer learning is to **take an already well-performing model on a problem-space similar to yours and then customising it to your use case**.\n",
    "\n",
    "Since we're working on a computer vision problem (image classification with FoodVision Mini), we can find pretrained classification models in [`torchvision.models`](https://pytorch.org/vision/stable/models.html#classification).\n",
    "\n",
    "Exploring the documentation, you'll find plenty of common computer vision architecture backbones such as:\n",
    "\n",
    "| **Architecuture backbone** | **Code** |\n",
    "| ----- | ----- |\n",
    "| [ResNet](https://arxiv.org/abs/1512.03385)'s | `torchvision.models.resnet18()`, `torchvision.models.resnet50()`... | \n",
    "| [VGG](https://arxiv.org/abs/1409.1556) (similar to what we used for TinyVGG) | `torchvision.models.vgg16()` | \n",
    "| [EfficientNet](https://arxiv.org/abs/1905.11946)'s | `torchvision.models.efficientnet_b0()`, `torchvision.models.efficientnet_b1()`... | \n",
    "| [VisionTransformer](https://arxiv.org/abs/2010.11929) (ViT's)| `torchvision.models.vit_b_16()`, `torchvision.models.vit_b_32()`... | \n",
    "| [ConvNeXt](https://arxiv.org/abs/2201.03545) | `torchvision.models.convnext_tiny()`,  `torchvision.models.convnext_small()`... |\n",
    "| More available in `torchvision.models` | `torchvision.models...` | "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874d4a5f-5b6c-44ad-b981-d4ba4eaca8ab",
   "metadata": {},
   "source": [
    "### 3.1 Which pretrained model should you use?\n",
    "\n",
    "It depends on your problem/the device you're working with.\n",
    "\n",
    "Generally, the higher number in the model name (e.g. `efficientnet_b0()` -> `efficientnet_b1()` -> `efficientnet_b7()`) means *better performance* but a *larger* model.\n",
    "\n",
    "You might think better performance is *always better*, right?\n",
    "\n",
    "That's true but **some better performing models are too big for some devices**.\n",
    "\n",
    "For example, say you'd like to run your model on a mobile-device, you'll have to take into account the limited compute resources on the device, thus you'd be looking for a smaller model.\n",
    "\n",
    "But if you've got unlimited compute power, as [*The Bitter Lesson*](http://www.incompleteideas.net/IncIdeas/BitterLesson.html) states, you'd likely take the biggest, most compute hungry model you can.\n",
    "\n",
    "Understanding this **performance vs. speed vs. size tradeoff** will come with time and practice.\n",
    "\n",
    "For me, I've found a nice balance in the `efficientnet_bX` models. \n",
    "\n",
    "As of May 2022, [Nutrify](https://nutrify.app) (the machine learning powered app I'm working on) is powered by an `efficientnet_b0`.\n",
    "\n",
    "[Comma.ai](https://comma.ai/) (a company that makes open source self-driving car software) [uses an `efficientnet_b2`](https://geohot.github.io/blog/jekyll/update/2021/10/29/an-architecture-for-life.html) to learn a representation of the road.\n",
    "\n",
    "> **Note:** Even though we're using `efficientnet_bX`, it's important not to get too attached to any one architecture, as they are always changing as new research gets released. Best to experiment, experiment, experiment and see what works for your problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ff4cf6-0de2-4f85-80fc-dbcff29d1ccc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
